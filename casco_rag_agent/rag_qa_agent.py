#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
RAG-Anything é—®ç­”æ™ºèƒ½ä½“
åŸºäºå®˜æ–¹ç¤ºä¾‹ä¸¥æ ¼å®ç°
"""

import asyncio
import json
import os
from pathlib import Path
from typing import List, Dict, Any
from dotenv import load_dotenv, find_dotenv
from tqdm import tqdm
import logging

from raganything import RAGAnything, RAGAnythingConfig
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(find_dotenv())


class RAGQAAgent:
    """åŸºäº RAG-Anything çš„é—®ç­”æ™ºèƒ½ä½“"""
    
    @staticmethod
    def _get_embedding_dim(model_name: str) -> int:
        """æ ¹æ®æ¨¡å‹åç§°è·å– embedding ç»´åº¦"""
        dim_map = {
            'text-embedding-3-large': 3072,
            'text-embedding-3-small': 1536,
            'text-embedding-ada-002': 1536,
            'bge-m3': 1024,
            'bge-large': 1024,
            'bge-base': 768,
            'bge-small': 512,
        }
        
        # ç²¾ç¡®åŒ¹é…
        if model_name in dim_map:
            return dim_map[model_name]
        
        # æ¨¡ç³ŠåŒ¹é…
        model_lower = model_name.lower()
        for key, dim in dim_map.items():
            if key in model_lower:
                return dim
        
        # é»˜è®¤è¿”å›æœ€å¸¸ç”¨çš„ç»´åº¦
        logger.warning(f"æœªçŸ¥çš„ embedding æ¨¡å‹: {model_name}ï¼Œä½¿ç”¨é»˜è®¤ç»´åº¦ 1536")
        return 1536
    
    def __init__(
        self,
        working_dir: str = "./rag_storage",
        parser: str = "mineru",
        parse_method: str = "auto"
    ):
        """
        åˆå§‹åŒ– RAG é—®ç­”æ™ºèƒ½ä½“
        
        Args:
            working_dir: RAG å­˜å‚¨ç›®å½•
            parser: è§£æå™¨é€‰æ‹© (mineru æˆ– docling)
            parse_method: è§£ææ–¹æ³• (auto, ocr æˆ– txt)
        """
        # LLM API é…ç½®
        self.llm_api_key = os.getenv("CLOUD_API_KEY")
        self.llm_base_url = os.getenv("CLOUD_BASE_URL")
        self.llm_model = os.getenv("CLOUD_MODEL", "gpt-4o-mini")
        print("print",self.llm_model)
        # Vision Model é…ç½®ï¼ˆç”¨äºå¤„ç†å›¾åƒã€è¡¨æ ¼ç­‰å¤šæ¨¡æ€å†…å®¹ï¼‰
        # å¦‚æœä¸è®¾ç½®ï¼Œé»˜è®¤ä½¿ç”¨ LLM çš„é…ç½®
        self.vision_model = os.getenv("VISION_MODEL") or self.llm_model
        self.vision_api_key = os.getenv("VISION_API_KEY") or self.llm_api_key
        self.vision_base_url = os.getenv("VISION_BASE_URL") or self.llm_base_url
        
        # Embedding API é…ç½®ï¼ˆå¯èƒ½ä¸ LLM ä¸åŒï¼‰
        self.embedding_api_key = os.getenv("OPENAI_API_KEY")
        self.embedding_base_url = os.getenv("OPENAI_BASE_URL")
        self.embedding_model = os.getenv("OPENAI_API_MODEL", "text-embedding-3-large")
        
        # Embedding ç»´åº¦é…ç½®ï¼ˆæ ¹æ®æ¨¡å‹è‡ªåŠ¨è®¾ç½®ï¼‰
        self.embedding_dim = self._get_embedding_dim(self.embedding_model)
        
        if not self.llm_api_key:
            raise ValueError("æœªæ‰¾åˆ° LLM API Keyï¼Œè¯·è®¾ç½® CLOUD_API_KEY ç¯å¢ƒå˜é‡")
        
        if not self.embedding_api_key:
            raise ValueError("æœªæ‰¾åˆ° Embedding API Keyï¼Œè¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        
        # åˆ›å»º RAGAnything é…ç½®
        self.config = RAGAnythingConfig(
            working_dir=working_dir,
            parser=parser,
            parse_method=parse_method,
            enable_image_processing=True,
            enable_table_processing=True,
            enable_equation_processing=True,
        )
        
        # åˆå§‹åŒ– RAG
        self.rag = None
        
    def _get_llm_model_func(self):
        """åˆ›å»º LLM æ¨¡å‹å‡½æ•°ï¼ˆpickle-safeï¼‰"""
        # æ•è·å˜é‡åˆ°å±€éƒ¨ä½œç”¨åŸŸï¼Œé¿å… pickle é—®é¢˜
        llm_model = self.llm_model
        llm_api_key = self.llm_api_key
        llm_base_url = self.llm_base_url
        
        def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
            """LLM æ¨¡å‹å‡½æ•° - ç‹¬ç«‹å‡½æ•°ï¼Œå¯ä»¥è¢« pickle"""
            return openai_complete_if_cache(
                llm_model,
                prompt,
                system_prompt=system_prompt,
                history_messages=history_messages,
                api_key=llm_api_key,
                base_url=llm_base_url,
                **kwargs,
            )
        
        return llm_model_func
    
    def _get_vision_model_func(self):
        """åˆ›å»ºè§†è§‰æ¨¡å‹å‡½æ•°ï¼ˆpickle-safeï¼‰"""
        # æ•è·å˜é‡åˆ°å±€éƒ¨ä½œç”¨åŸŸ
        vision_model = self.vision_model
        vision_api_key = self.vision_api_key
        vision_base_url = self.vision_base_url
        llm_model_func = self._get_llm_model_func()  # è·å– LLM å‡½æ•°
        
        def vision_model_func(
            prompt,
            system_prompt=None,
            history_messages=[],
            image_data=None,
            messages=None,
            **kwargs
        ):
            """è§†è§‰æ¨¡å‹å‡½æ•° - ç‹¬ç«‹å‡½æ•°ï¼Œå¯ä»¥è¢« pickle"""
            # å¦‚æœæä¾›äº†messagesæ ¼å¼ï¼ˆç”¨äºå¤šæ¨¡æ€VLMå¢å¼ºæŸ¥è¯¢ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
            if messages:
                return openai_complete_if_cache(
                    vision_model,
                    "",
                    system_prompt=None,
                    history_messages=[],
                    messages=messages,
                    api_key=vision_api_key,
                    base_url=vision_base_url,
                    **kwargs,
                )
            # ä¼ ç»Ÿå•å›¾ç‰‡æ ¼å¼
            elif image_data:
                return openai_complete_if_cache(
                    vision_model,
                    "",
                    system_prompt=None,
                    history_messages=[],
                    messages=[
                        {"role": "system", "content": system_prompt}
                        if system_prompt
                        else None,
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": prompt},
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/jpeg;base64,{image_data}"
                                    },
                                },
                            ],
                        }
                        if image_data
                        else {"role": "user", "content": prompt},
                    ],
                    api_key=vision_api_key,
                    base_url=vision_base_url,
                    **kwargs,
                )
            # çº¯æ–‡æœ¬æ ¼å¼
            else:
                return llm_model_func(prompt, system_prompt, history_messages, **kwargs)
        
        return vision_model_func
    
    def _get_embedding_func(self):
        """åˆ›å»ºåµŒå…¥å‡½æ•° - ä½¿ç”¨ç‹¬ç«‹çš„ Embedding API é…ç½®"""
        logger.info(f"é…ç½® Embedding API:")
        logger.info(f"  Base URL: {self.embedding_base_url}")
        logger.info(f"  Model: {self.embedding_model}")
        logger.info(f"  Dimension: {self.embedding_dim}")
        logger.info(f"  API Key: {self.embedding_api_key[:10]}..." if self.embedding_api_key else "  API Key: æœªè®¾ç½®")
        
        return EmbeddingFunc(
            embedding_dim=self.embedding_dim,  # ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹çš„ç»´åº¦
            max_token_size=8192,
            func=lambda texts: openai_embed(
                texts,
                model=self.embedding_model,
                api_key=self.embedding_api_key,  # ä½¿ç”¨ Embedding ä¸“ç”¨çš„ API Key
                base_url=self.embedding_base_url,  # ä½¿ç”¨ Embedding ä¸“ç”¨çš„ Base URL
            ),
        )
    
    async def initialize(self):
        """åˆå§‹åŒ– RAG ç³»ç»Ÿ"""
        logger.info("ğŸš€ åˆå§‹åŒ– RAG-Anything ç³»ç»Ÿ...")
        logger.info(f"LLM API: {self.llm_base_url} / {self.llm_model}")
        logger.info(f"Vision API: {self.vision_base_url} / {self.vision_model}")
        logger.info(f"Embedding API: {self.embedding_base_url} / {self.embedding_model} ({self.embedding_dim}ç»´)")
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ—§æ•°æ®ä¸”ç»´åº¦å¯èƒ½ä¸åŒ¹é…
        storage_path = Path(self.config.working_dir)
        if storage_path.exists():
            kv_files = list(storage_path.glob("vdb_*.json"))
            if kv_files:
                logger.warning("âš ï¸  æ£€æµ‹åˆ°å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“")
                logger.warning(f"   å¦‚æœé‡åˆ°ç»´åº¦ä¸åŒ¹é…é”™è¯¯ï¼Œè¯·è¿è¡Œ:")
                logger.warning(f"   rm -rf {self.config.working_dir}")
                logger.warning(f"   mkdir -p {self.config.working_dir}")
        
        # åˆå§‹åŒ– RAGAnything
        # æ³¨æ„ï¼šå¿…é¡»ä½¿ç”¨ç‹¬ç«‹å‡½æ•°è€Œä¸æ˜¯ç±»æ–¹æ³•ï¼Œå› ä¸º LightRAG ä½¿ç”¨å¤šè¿›ç¨‹éœ€è¦ pickle
        self.rag = RAGAnything(
            config=self.config,
            llm_model_func=self._get_llm_model_func(),  # è¿”å›ç‹¬ç«‹å‡½æ•°
            vision_model_func=self._get_vision_model_func(),  # è¿”å›ç‹¬ç«‹å‡½æ•°
            embedding_func=self._get_embedding_func(),
        )
        
        logger.info("âœ… RAG-Anything ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    async def process_document(
        self,
        file_path: str,
        output_dir: str = "./output",
        parse_method: str = None,
        show_progress: bool = True,
    ):
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£
        
        Args:
            file_path: æ–‡æ¡£è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            parse_method: è§£ææ–¹æ³• (å¯é€‰)
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
            device: ä½¿ç”¨çš„è®¾å¤‡ ("cpu", "cuda", "mps" ç­‰)
        """
        if not self.rag:
            await self.initialize()
        
        file_name = Path(file_path).name
        logger.info(f"ğŸ“„ å¼€å§‹å¤„ç†: {file_name} (è®¾å¤‡: {device})")
        
        if show_progress:
            with tqdm(total=100, desc=f"å¤„ç† {file_name[:50]}", ncols=100, 
                     bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:
                # è§£æé˜¶æ®µ
                pbar.set_description(f"è§£æ {file_name[:50]}")
                pbar.update(30)
                
                # å¤„ç†æ–‡æ¡£ï¼ˆæ˜ç¡®æŒ‡å®šä½¿ç”¨ CPUï¼‰
                await self.rag.process_document_complete(
                    file_path=file_path,
                    output_dir=output_dir,
                    parse_method=parse_method or self.config.parse_method,
                )
                
                # å®Œæˆ
                pbar.set_description(f"å®Œæˆ {file_name[:50]}")
                pbar.update(70)
        else:
            await self.rag.process_document_complete(
                file_path=file_path,
                output_dir=output_dir,
                parse_method=parse_method or self.config.parse_method,
            )
        
        logger.info(f"âœ… å¤„ç†å®Œæˆ: {file_name}")
    
    async def process_folder(
        self,
        folder_path: str,
        output_dir: str = "./output",
        file_extensions: List[str] = None,
        recursive: bool = True,
        max_workers: int = 4,
        show_progress: bool = True
    ):
        """
        æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ–‡æ¡£
        
        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            file_extensions: æ”¯æŒçš„æ–‡ä»¶æ‰©å±•ååˆ—è¡¨
            recursive: æ˜¯å¦é€’å½’å¤„ç†å­æ–‡ä»¶å¤¹
            max_workers: æœ€å¤§å¹¶è¡Œå¤„ç†æ•°
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        """
        if not self.rag:
            await self.initialize()
        
        logger.info(f"ğŸ“ æ‰«ææ–‡ä»¶å¤¹: {folder_path}")
        
        if file_extensions is None:
            file_extensions = [".pdf", ".docx", ".pptx"]
        
        # å…ˆç»Ÿè®¡æ–‡ä»¶æ•°é‡
        folder_path_obj = Path(folder_path)
        if recursive:
            files = []
            for ext in file_extensions:
                files.extend(list(folder_path_obj.rglob(f"*{ext}")))
        else:
            files = []
            for ext in file_extensions:
                files.extend(list(folder_path_obj.glob(f"*{ext}")))
        
        total_files = len(files)
        logger.info(f"ğŸ“Š æ‰¾åˆ° {total_files} ä¸ªæ–‡ä»¶å¾…å¤„ç†")
        
        if total_files == 0:
            logger.warning("âš ï¸  æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶")
            return
        
        # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
        logger.info("æ–‡ä»¶åˆ—è¡¨:")
        for i, f in enumerate(files[:10], 1):
            logger.info(f"  {i}. {f.name}")
        if total_files > 10:
            logger.info(f"  ... è¿˜æœ‰ {total_files - 10} ä¸ªæ–‡ä»¶")
        
        # å¤„ç†å¤šä¸ªæ–‡æ¡£ï¼ˆå¸¦æ€»è¿›åº¦æ¡ï¼‰
        if show_progress:
            print(f"\n{'='*80}")
            print(f"å¼€å§‹æ‰¹é‡å¤„ç† {total_files} ä¸ªæ–‡ä»¶ (å¹¶è¡Œæ•°: {max_workers})")
            print(f"{'='*80}\n")
            
            # åˆ›å»ºä¸€ä¸ªå¤§çš„è¿›åº¦æ¡æ˜¾ç¤ºæ•´ä½“è¿›åº¦
            with tqdm(total=total_files, desc="æ€»ä½“è¿›åº¦", unit="æ–‡ä»¶", 
                     ncols=100, position=0, leave=True,
                     bar_format='{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as main_pbar:
                
                # è®°å½•å·²å¤„ç†æ–‡ä»¶æ•°
                processed_count = 0
                
                # é€ä¸ªå¤„ç†æ–‡ä»¶
                for file_path in files:
                    try:
                        # æ›´æ–°å½“å‰å¤„ç†çš„æ–‡ä»¶å
                        file_name = file_path.name
                        main_pbar.set_description(f"å¤„ç†ä¸­: {file_name[:40]}")
                        
                        # å¤„ç†æ–‡æ¡£
                        await self.rag.process_document_complete(
                            file_path=str(file_path),
                            output_dir=output_dir,
                            parse_method=self.config.parse_method
                        )
                        
                        processed_count += 1
                        main_pbar.update(1)
                        main_pbar.set_description(f"å®Œæˆ {processed_count}/{total_files}")
                        
                    except Exception as e:
                        logger.error(f"âŒ å¤„ç†å¤±è´¥: {file_name} - {e}")
                        main_pbar.update(1)
                        continue
                
                main_pbar.set_description(f"âœ… å…¨éƒ¨å®Œæˆ")
        else:
            await self.rag.process_folder_complete(
                folder_path=folder_path,
                output_dir=output_dir,
                file_extensions=file_extensions,
                recursive=recursive,
                max_workers=max_workers
            )
        
        logger.info(f"âœ… æ–‡ä»¶å¤¹å¤„ç†å®Œæˆ: {folder_path}")
        logger.info(f"âœ… å…±å¤„ç† {total_files} ä¸ªæ–‡ä»¶")
    
    async def query(
        self,
        question: str,
        mode: str = "hybrid",
        return_context: bool = True,
        show_progress: bool = True
    ) -> Dict[str, Any]:
        """
        æŸ¥è¯¢ RAG ç³»ç»Ÿ
        
        Args:
            question: é—®é¢˜
            mode: æŸ¥è¯¢æ¨¡å¼ (hybrid, local, global, naive)
            return_context: æ˜¯å¦è¿”å›æ£€ç´¢ä¸Šä¸‹æ–‡
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            åŒ…å«é—®é¢˜ã€ä¸Šä¸‹æ–‡å’Œç­”æ¡ˆçš„å­—å…¸
        """
        if not self.rag:
            await self.initialize()
        
        logger.info(f"â“ é—®é¢˜: {question}")
        logger.info(f"ğŸ” æŸ¥è¯¢æ¨¡å¼: {mode}")
        
        # æ‰§è¡ŒæŸ¥è¯¢
        if show_progress:
            with tqdm(total=100, desc="æŸ¥è¯¢ä¸­", ncols=100,
                     bar_format='{l_bar}{bar}| [{elapsed}<{remaining}]') as pbar:
                pbar.set_description("æ£€ç´¢ç›¸å…³å†…å®¹")
                pbar.update(30)
                
                result = await self.rag.aquery(
                    question,
                    mode=mode
                )
                
                pbar.set_description("ç”Ÿæˆç­”æ¡ˆ")
                pbar.update(40)
                
                pbar.set_description("å®Œæˆ")
                pbar.update(30)
        else:
            result = await self.rag.aquery(
                question,
                mode=mode
            )
        
        # æ„é€ è¿”å›ç»“æœ
        response = {
            "question": question,
            "answer": result
        }
        
        # å¦‚æœéœ€è¦è¿”å›ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥ä»ç»“æœä¸­æå–
        if return_context:
            # æ³¨æ„ï¼šè¿™é‡Œçš„ retrieved_contexts æ˜¯ç¤ºä¾‹ï¼Œ
            # å®é™…çš„ä¸Šä¸‹æ–‡æå–éœ€è¦æ ¹æ® RAG-Anything çš„å…·ä½“å®ç°æ¥è°ƒæ•´
            response["retrieved_contexts"] = []
        
        # æ˜¾ç¤ºç­”æ¡ˆï¼ˆæˆªæ–­è¿‡é•¿çš„ç­”æ¡ˆï¼‰
        answer_preview = result[:200] + "..." if len(result) > 200 else result
        logger.info(f"ğŸ’¡ ç­”æ¡ˆ: {answer_preview}")
        
        return response
    
    async def query_batch(
        self,
        questions: List[str],
        mode: str = "hybrid",
        show_progress: bool = True
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡æŸ¥è¯¢
        
        Args:
            questions: é—®é¢˜åˆ—è¡¨
            mode: æŸ¥è¯¢æ¨¡å¼
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            ç»“æœåˆ—è¡¨
        """
        results = []
        logger.info(f"ğŸ“ å¼€å§‹æ‰¹é‡æŸ¥è¯¢ {len(questions)} ä¸ªé—®é¢˜")
        
        if show_progress:
            for question in tqdm(questions, desc="æ‰¹é‡æŸ¥è¯¢è¿›åº¦", ncols=100):
                result = await self.query(question, mode=mode, show_progress=False)
                results.append(result)
        else:
            for i, question in enumerate(questions, 1):
                logger.info(f"\n{'='*60}")
                logger.info(f"å¤„ç†é—®é¢˜ {i}/{len(questions)}")
                result = await self.query(question, mode=mode, show_progress=False)
                results.append(result)
        
        logger.info(f"âœ… æ‰¹é‡æŸ¥è¯¢å®Œæˆï¼Œå…± {len(results)} ä¸ªç»“æœ")
        return results
    
    def save_results(
        self,
        results: List[Dict[str, Any]],
        output_file: str = "qa_results.json"
    ):
        """
        ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶ï¼ˆæŒ‰ç…§ç¤ºä¾‹æ¨¡æ¿æ ¼å¼ï¼‰
        
        Args:
            results: ç»“æœåˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        # æŒ‰ç…§ç¤ºä¾‹æ¨¡æ¿æ ¼å¼æ„é€ è¾“å‡º
        output_data = {
            "items": results
        }
        
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


async def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ– RAG é—®ç­”æ™ºèƒ½ä½“
    agent = RAGQAAgent(
        working_dir="./rag_storage",
        parser="mineru",
        parse_method="auto"
    )
    
    # 1. å¤„ç†æ–‡æ¡£ï¼ˆæ‰¹é‡å¤„ç† data ç›®å½•ä¸‹çš„æ‰€æœ‰ PDFï¼‰
    data_dir = "/home/honglianglu/ssd/casco/data"
    print("=" * 60)
    print("å¼€å§‹å¤„ç†æ–‡æ¡£...")
    print("=" * 60)
    
    await agent.process_folder(
        folder_path=data_dir,
        output_dir="./output",
        file_extensions=[".pdf"],
        recursive=True,
        max_workers=2  # æ ¹æ®æ‚¨çš„ç³»ç»Ÿèµ„æºè°ƒæ•´
    )
    
    # 2. ç¤ºä¾‹æŸ¥è¯¢
    print("\n" + "=" * 60)
    print("å¼€å§‹é—®ç­”æµ‹è¯•...")
    print("=" * 60)
    
    questions = [
        "æ–‡æ¡£ä¸­æåˆ°çš„ä¸»è¦å®‰å…¨è§„èŒƒæœ‰å“ªäº›ï¼Ÿ",
        "CBTCç³»ç»Ÿçš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†æ˜¯ä»€ä¹ˆï¼Ÿ",
        "2023å¹´åŒ—äº¬åœ°é“æ˜Œå¹³çº¿äº‹æ•…çš„ä¸»è¦åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ",
    ]
    
    # æ‰¹é‡æŸ¥è¯¢
    results = await agent.query_batch(questions, mode="hybrid")
    
    # 3. ä¿å­˜ç»“æœ
    agent.save_results(
        results,
        output_file="/home/honglianglu/ssd/casco/qa_results.json"
    )
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())

