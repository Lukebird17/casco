#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
@File    :   enhanced_utils.py
@Time    :   2025/11/15
@Desc    :   å¢å¼ºçš„æ–‡æ¡£å¤„ç†å·¥å…·ï¼Œæ”¯æŒè¡¨æ ¼æå–å’Œç»“æ„åŒ–ä¿¡æ¯ä¿ç•™
'''

import os
import re
from typing import Dict, List, Tuple
import json
from tqdm import tqdm
import tiktoken
import fitz  # PyMuPDF
from PIL import Image
import io
import html
from bs4 import BeautifulSoup

enc = tiktoken.get_encoding("cl100k_base")


class EnhancedReadFiles:
    """
    å¢å¼ºçš„æ–‡ä»¶è¯»å–ç±»ï¼Œæ”¯æŒè¡¨æ ¼æå–å’Œå…ƒæ•°æ®ä¿ç•™
    """
    
    # ç±»çº§åˆ«çš„OCRç³»ç»Ÿå®ä¾‹ï¼ˆå…¨å±€å…±äº«ï¼Œåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
    _ocr_system = None

    def __init__(self, path: str) -> None:
        self._path = path
        self.file_list = self.get_files()
        
        # å¦‚æœOCRç³»ç»Ÿè¿˜æ²¡åˆå§‹åŒ–ï¼Œåˆ™åˆå§‹åŒ–ä¸€æ¬¡
        if EnhancedReadFiles._ocr_system is None:
            print("ğŸš€ åˆå§‹åŒ–OCRç³»ç»Ÿï¼ˆå…¨å±€å•ä¾‹ï¼‰...")
            from simple_ocr_system import SimpleOCRSystem
            EnhancedReadFiles._ocr_system = SimpleOCRSystem()
            print("âœ… OCRç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")

    def get_files(self):
        file_list = []
        for filepath, dirnames, filenames in os.walk(self._path):
            for filename in filenames:
                if filename.endswith((".md", ".txt", ".pdf")):
                    file_list.append(os.path.join(filepath, filename))
        # print(file_list)
        return file_list

    def get_content(self, max_token_len: int = 600, cover_content: int = 150):
        """
        è·å–æ‰€æœ‰æ–‡æ¡£å†…å®¹ï¼Œä¿ç•™å…ƒæ•°æ®
        """
        docs = []
        for file in tqdm(self.file_list, desc="Processing files"):
            try:
                # è¯»å–æ–‡ä»¶å†…å®¹å’Œå…ƒæ•°æ®
                content, metadata = self.read_file_content_with_metadata(file)
                # print(content)
                # åˆ†å—å¹¶é™„åŠ å…ƒæ•°æ®
                chunk_content = self.get_chunk_with_metadata(
                    content, metadata, 
                    max_token_len=max_token_len, 
                    cover_content=cover_content
                )
                docs.extend(chunk_content)
            except Exception as e:
                print(f"Error processing {file}: {e}")
                continue
        
        return docs

    @classmethod
    def read_file_content_with_metadata(cls, file_path: str) -> Tuple[str, Dict]:
        """
        è¯»å–æ–‡ä»¶å†…å®¹å¹¶æå–å…ƒæ•°æ®
        """
        metadata = {
            'source': file_path,
            'filename': os.path.basename(file_path),
            'file_type': os.path.splitext(file_path)[1]
        }
        
        if file_path.endswith('.pdf'):
            content = cls.read_pdf_enhanced(file_path, metadata)
        elif file_path.endswith('.md'):
            content = cls.read_markdown(file_path)
        elif file_path.endswith('.txt'):
            content = cls.read_text(file_path)
        else:
            raise ValueError("Unsupported file type")
        
        return content, metadata

    @classmethod
    def read_pdf_enhanced(cls, file_path: str, metadata: Dict) -> str:
        """
        å¢å¼ºçš„PDFè¯»å–ï¼Œä½¿ç”¨PaddleOCRæ”¯æŒè¡¨æ ¼ã€å¤šè¯­è¨€ã€æ—‹è½¬çº æ­£ç­‰
        Args:
            file_path: PDFæ–‡ä»¶è·¯å¾„
            metadata: å…ƒæ•°æ®å­—å…¸
        Returns:
            æå–çš„æ–‡æœ¬
        """
        # ä½¿ç”¨ç±»çº§åˆ«çš„OCRç³»ç»Ÿå®ä¾‹ï¼ˆå·²ç»åˆå§‹åŒ–å¥½äº†ï¼‰
        ocr_system = cls._ocr_system
        
        # ä½¿ç”¨PPStructureV3è¿›è¡Œæ–‡æ¡£ç»“æ„åˆ†æ
        results = ocr_system.process_document(
            file_path,
            use_structure_analysis=True,  # ä½¿ç”¨ç»“æ„åˆ†æï¼ˆç‰ˆé¢+è¡¨æ ¼+OCRï¼‰
            extract_toc=True              # æå–ç›®å½•
        )
        print("read_pdf_enhanced",results)
        # åˆå¹¶æ‰€æœ‰é¡µé¢çš„æ–‡æœ¬
        full_text = []
        metadata['total_pages'] = len(results['pages'])
        
        for page in results['pages']:
            full_text.append(f"\n{'='*50}\n[ç¬¬ {page['page_number']} é¡µ]\n{'='*50}\n")
            
            # æ·»åŠ è¡¨æ ¼ï¼ˆMarkdownæ ¼å¼ï¼‰
            # æ³¨æ„ï¼šAPIå¤„ç†çš„ç»“æœå¯èƒ½æ²¡æœ‰'tables'å­—æ®µï¼Œéœ€è¦æ£€æŸ¥
            if 'tables' in page and page['tables']:
                for table in page['tables']:
                    if table.get('markdown'):
                        full_text.append(f"\n[è¡¨æ ¼ {table['table_index']}]\n{table['markdown']}\n")
            
            # æ·»åŠ æ–‡æœ¬
            if page.get('text'):
                full_text.append(page['text'])
        
        return "\n".join(full_text)

    @classmethod
    def detect_tables_in_page(cls, page) -> List[str]:
        """
        æ£€æµ‹é¡µé¢ä¸­çš„è¡¨æ ¼å¹¶æå–
        ä½¿ç”¨ç®€å•çš„å¯å‘å¼æ–¹æ³•æ£€æµ‹è¡¨æ ¼ç»“æ„
        """
        tables = []
        text = page.get_text("text")
        lines = text.split('\n')
        
        # ç®€å•çš„è¡¨æ ¼æ£€æµ‹ï¼šè¿ç»­å¤šè¡ŒåŒ…å«å¤šä¸ªç©ºæ ¼åˆ†éš”çš„å†…å®¹
        potential_table = []
        in_table = False
        
        for line in lines:
            # æ£€æµ‹æ˜¯å¦å¯èƒ½æ˜¯è¡¨æ ¼è¡Œï¼ˆåŒ…å«å¤šä¸ªè¿ç»­ç©ºæ ¼æˆ–åˆ¶è¡¨ç¬¦ï¼‰
            if re.search(r'\s{2,}|\t', line) and len(line.split()) >= 2:
                potential_table.append(line)
                in_table = True
            elif in_table:
                # å¦‚æœè¡¨æ ¼ä¸­æ–­ï¼Œä¿å­˜å½“å‰è¡¨æ ¼
                if len(potential_table) >= 3:  # è‡³å°‘3è¡Œæ‰ç®—è¡¨æ ¼
                    tables.append('\n'.join(potential_table))
                potential_table = []
                in_table = False
        
        # ä¿å­˜æœ€åä¸€ä¸ªè¡¨æ ¼
        if len(potential_table) >= 3:
            tables.append('\n'.join(potential_table))
        
        return tables

    @classmethod
    def read_markdown(cls, file_path: str) -> str:
        """
        å¢å¼ºçš„Markdownè¯»å–ï¼Œç‰¹åˆ«å¤„ç†HTMLè¡¨æ ¼
        """
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()
        
        # å¤„ç†HTMLè¡¨æ ¼ï¼Œä½¿å…¶æ›´æ˜“æ£€ç´¢
        content = cls.enhance_markdown_tables(content)
        return content
    
    @classmethod
    def enhance_markdown_tables(cls, content: str) -> str:
        """
        å¢å¼ºHTMLè¡¨æ ¼çš„å¯è¯»æ€§å’Œå¯æ£€ç´¢æ€§
        1. æå–è¡¨æ ¼å‰çš„æ ‡é¢˜/è¯´æ˜
        2. å°†HTMLè¡¨æ ¼è½¬æ¢ä¸ºæ›´æ˜“æ£€ç´¢çš„æ ¼å¼
        3. ä¿ç•™è¡¨æ ¼çš„è¯­ä¹‰å®Œæ•´æ€§
        """
        # æŸ¥æ‰¾æ‰€æœ‰HTMLè¡¨æ ¼åŠå…¶ä¸Šä¸‹æ–‡
        # åŒ¹é…ï¼š<div>è¡¨X</div> + å¯èƒ½çš„ç©ºè¡Œ + <table>...</table>
        table_pattern = r'(<div[^>]*>.*?è¡¨\s*\d+.*?</div>\s*\n*\s*)?(<table[^>]*>.*?</table>)'
        
        enhanced_content = content
        tables = re.finditer(table_pattern, content, re.DOTALL | re.IGNORECASE)
        
        for match in tables:
            title_html = match.group(1) if match.group(1) else ''
            table_html = match.group(2)
            full_match = match.group(0)
            
            # è§£æè¡¨æ ¼æ ‡é¢˜
            table_title = ''
            if title_html:
                soup_title = BeautifulSoup(title_html, 'html.parser')
                table_title = soup_title.get_text().strip()
            
            # è§£æHTMLè¡¨æ ¼
            try:
                soup = BeautifulSoup(table_html, 'html.parser')
                table = soup.find('table')
                
                if table:
                    # æå–è¡¨æ ¼ä¸ºç»“æ„åŒ–æ–‡æœ¬
                    table_text = cls.html_table_to_text(table, table_title)
                    
                    # æ›¿æ¢åŸå§‹HTMLè¡¨æ ¼
                    enhanced_content = enhanced_content.replace(full_match, table_text)
            except Exception as e:
                print(f"Warning: Failed to parse table: {e}")
                continue
        
        return enhanced_content
    
    @classmethod
    def html_table_to_text(cls, table_soup, table_title: str = '') -> str:
        """
        å°†HTMLè¡¨æ ¼è½¬æ¢ä¸ºæ˜“äºæ£€ç´¢çš„æ–‡æœ¬æ ¼å¼
        ä¿ç•™å®Œæ•´çš„è¡¨æ ¼ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯
        """
        lines = []
        
        # æ·»åŠ è¡¨æ ¼æ ‡é¢˜
        if table_title:
            lines.append(f"\n{'='*60}")
            lines.append(f"ã€{table_title}ã€‘")
            lines.append(f"{'='*60}\n")
        
        # æå–è¡¨å¤´
        headers = []
        thead = table_soup.find('thead')
        if thead:
            header_rows = thead.find_all('tr')
            for row in header_rows:
                cells = row.find_all(['th', 'td'])
                headers = [cell.get_text().strip() for cell in cells]
        else:
            # å¦‚æœæ²¡æœ‰theadï¼Œå°è¯•ä»ç¬¬ä¸€è¡Œæå–
            first_row = table_soup.find('tr')
            if first_row:
                cells = first_row.find_all(['th', 'td'])
                if any(cell.name == 'th' for cell in cells):
                    headers = [cell.get_text().strip() for cell in cells]
        
        # æå–æ‰€æœ‰è¡Œ
        rows = []
        tbody = table_soup.find('tbody') or table_soup
        for row in tbody.find_all('tr'):
            cells = row.find_all(['td', 'th'])
            row_data = []
            for cell in cells:
                # å¤„ç†colspanå’Œrowspan
                colspan = int(cell.get('colspan', 1))
                text = cell.get_text().strip()
                row_data.append(text)
                # ä¸ºcolspanæ·»åŠ å ä½
                for _ in range(colspan - 1):
                    row_data.append('')
            if row_data:
                rows.append(row_data)
        
        # å¦‚æœæœ‰è¡¨å¤´ï¼Œå•ç‹¬æ ‡è®°
        if headers:
            lines.append("ã€è¡¨å¤´ã€‘")
            lines.append(" | ".join(headers))
            lines.append("-" * 80)
        
        # æ·»åŠ æ•°æ®è¡Œ
        lines.append("ã€è¡¨æ ¼æ•°æ®ã€‘")
        for i, row in enumerate(rows):
            # è·³è¿‡å¯èƒ½é‡å¤çš„è¡¨å¤´è¡Œ
            if headers and row == headers:
                continue
            
            # æ ¼å¼åŒ–æ¯ä¸€è¡Œ
            if headers and len(row) == len(headers):
                # é”®å€¼å¯¹æ ¼å¼ï¼Œæ›´æ˜“ç†è§£
                row_text = " | ".join([f"{h}:{v}" if v else h for h, v in zip(headers, row)])
            else:
                # ç®€å•æ ¼å¼
                row_text = " | ".join(row)
            
            lines.append(f"  {row_text}")
        
        lines.append("\n" + "="*60 + "\n")
        
        return "\n".join(lines)

    @classmethod
    def read_text(cls, file_path: str) -> str:
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.read()

    @classmethod
    def get_chunk_with_metadata(cls, text: str, metadata: Dict, 
                                max_token_len: int = 600, 
                                cover_content: int = 150) -> List[str]:
        """
        åˆ†å—å¹¶ä¿ç•™å…ƒæ•°æ®ä¿¡æ¯ï¼Œç¡®ä¿ä¸è¶…è¿‡tokené™åˆ¶
        """
        chunks = cls.get_chunk(text, max_token_len, cover_content)
        
        # ä¸ºæ¯ä¸ªchunkæ·»åŠ å…ƒæ•°æ®æ ‡è®°ï¼Œå¹¶éªŒè¯tokené•¿åº¦
        chunks_with_metadata = []
        metadata_header = f"[æ¥æº: {metadata['filename']}]"
        metadata_header_tokens = len(enc.encode(metadata_header))
        
        # å®é™…å¯ç”¨çš„tokené•¿åº¦ï¼ˆç•™100 tokenä½™é‡ï¼‰
        safe_max_tokens = 7900  # è¿œä½äº8000çš„ç¡¬é™åˆ¶
        
        for i, chunk in enumerate(chunks):
            chunk_with_meta = f"{metadata_header}\n{chunk}"
            chunk_tokens = len(enc.encode(chunk_with_meta))
            
            # å¦‚æœè¶…è¿‡å®‰å…¨é™åˆ¶ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
            if chunk_tokens > safe_max_tokens:
                print(f"âš ï¸  è­¦å‘Š: Chunk #{i+1} è¶…è¿‡é™åˆ¶ ({chunk_tokens} tokens)ï¼Œè¿›è¡Œå¼ºåˆ¶åˆ†å‰²...")
                
                # è®¡ç®—æ¯ä¸ªå­chunkçš„ç›®æ ‡tokenæ•°ï¼ˆç•™è¶³ä½™é‡ï¼‰
                sub_chunk_target_tokens = safe_max_tokens - metadata_header_tokens - 100
                
                # ä½¿ç”¨äºŒåˆ†æ³•æ‰¾åˆ°åˆé€‚çš„å­—ç¬¦æ•°
                lines = chunk.split('\n')
                current_lines = []
                current_tokens = 0
                
                for line in lines:
                    line_tokens = len(enc.encode(line + '\n'))
                    
                    if current_tokens + line_tokens <= sub_chunk_target_tokens:
                        current_lines.append(line)
                        current_tokens += line_tokens
                    else:
                        # ä¿å­˜å½“å‰å­chunk
                        if current_lines:
                            sub_chunk = '\n'.join(current_lines)
                            sub_chunk_with_meta = f"{metadata_header}\n{sub_chunk}"
                            
                            # æœ€åéªŒè¯
                            final_tokens = len(enc.encode(sub_chunk_with_meta))
                            if final_tokens > safe_max_tokens:
                                print(f"   âš ï¸  å­chunkä»è¶…é™({final_tokens} tokens)ï¼Œç»§ç»­ç»†åˆ†...")
                                # è¿›ä¸€æ­¥åˆ†å‰²å½“å‰è¡Œ
                                half = len(current_lines) // 2
                                if half > 0:
                                    sub_chunk = '\n'.join(current_lines[:half])
                                    sub_chunk_with_meta = f"{metadata_header}\n{sub_chunk}"
                                    chunks_with_metadata.append(sub_chunk_with_meta)
                                    current_lines = current_lines[half:]
                                else:
                                    # å•è¡Œå¤ªé•¿ï¼Œå¼ºåˆ¶æˆªæ–­
                                    chunks_with_metadata.append(sub_chunk_with_meta[:safe_max_tokens*4])
                                    current_lines = []
                            else:
                                chunks_with_metadata.append(sub_chunk_with_meta)
                        
                        # å¼€å§‹æ–°çš„å­chunk
                        current_lines = [line]
                        current_tokens = line_tokens
                
                # ä¿å­˜æœ€åä¸€ä¸ªå­chunk
                if current_lines:
                    sub_chunk = '\n'.join(current_lines)
                    sub_chunk_with_meta = f"{metadata_header}\n{sub_chunk}"
                    chunks_with_metadata.append(sub_chunk_with_meta)
            else:
                chunks_with_metadata.append(chunk_with_meta)
        
        return chunks_with_metadata

    @classmethod
    def get_chunk(cls, text: str, max_token_len: int = 600, cover_content: int = 150) -> List[str]:
        """
        æ™ºèƒ½åˆ†å—ï¼Œä¿ç•™æ®µè½å’Œè¡¨æ ¼å®Œæ•´æ€§
        """
        chunk_text = []
        curr_len = 0
        curr_chunk = ''
        token_len = max_token_len - cover_content
        
        # å…ˆè¯†åˆ«è¡¨æ ¼å—ï¼ˆä»¥ã€è¡¨å¤´ã€‘æˆ–ã€è¡¨æ ¼æ•°æ®ã€‘æ ‡è®°çš„å—ï¼‰
        table_pattern = r'(={60,}\nã€.*?è¡¨.*?ã€‘\n={60,}.*?(?=\n={60,}\n\n|\Z))'
        
        # åˆ†å‰²æ–‡æœ¬ï¼Œä¿ç•™è¡¨æ ¼å—çš„å®Œæ•´æ€§
        parts = re.split(table_pattern, text, flags=re.DOTALL)
        
        for part in parts:
            if not part or not part.strip():
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯è¡¨æ ¼å—
            is_table = 'ã€è¡¨å¤´ã€‘' in part or 'ã€è¡¨æ ¼æ•°æ®ã€‘' in part or part.startswith('='*60)
            
            if is_table:
                # è¡¨æ ¼å—å•ç‹¬å¤„ç†
                part_len = len(enc.encode(part))
                
                # å¦‚æœè¡¨æ ¼å¤ªå¤§ï¼Œéœ€è¦æ™ºèƒ½åˆ†å‰²
                if part_len > max_token_len:
                    # ä¿å­˜å½“å‰chunk
                    if curr_chunk:
                        chunk_text.append(curr_chunk)
                        curr_chunk = ''
                        curr_len = 0
                    
                    # æ™ºèƒ½åˆ†å‰²å¤§è¡¨æ ¼
                    table_chunks = cls.split_large_table(part, token_len)
                    chunk_text.extend(table_chunks)
                
                # è¡¨æ ¼èƒ½å®Œæ•´æ”¾å…¥å½“å‰chunk
                elif curr_len + part_len + 2 <= token_len:
                    if curr_chunk:
                        curr_chunk += '\n\n'
                        curr_len += 2
                    curr_chunk += part
                    curr_len += part_len
                
                # è¡¨æ ¼æ— æ³•æ”¾å…¥å½“å‰chunkï¼Œéœ€è¦æ–°chunk
                else:
                    if curr_chunk:
                        chunk_text.append(curr_chunk)
                    curr_chunk = part
                    curr_len = part_len
            
            else:
                # æ™®é€šæ–‡æœ¬æŒ‰æ®µè½åˆ†å‰²
                paragraphs = re.split(r'\n\s*\n', part)
                
                for para in paragraphs:
                    para = para.strip()
                    if not para:
                        continue
                        
                    para_len = len(enc.encode(para))
                    
                    # å¦‚æœå•ä¸ªæ®µè½è¶…é•¿ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
                    if para_len > max_token_len:
                        if curr_chunk:
                            chunk_text.append(curr_chunk)
                            curr_chunk = ''
                            curr_len = 0
                        
                        # åˆ†å‰²é•¿æ®µè½
                        sub_chunks = cls.split_long_paragraph(para, token_len)
                        chunk_text.extend(sub_chunks)
                        
                    elif curr_len + para_len + 2 <= token_len:
                        if curr_chunk:
                            curr_chunk += '\n\n'
                            curr_len += 2
                        curr_chunk += para
                        curr_len += para_len
                    else:
                        if curr_chunk:
                            chunk_text.append(curr_chunk)
                        
                        # æ·»åŠ è¦†ç›–å†…å®¹
                        if chunk_text and cover_content > 0:
                            prev_chunk = chunk_text[-1]
                            cover_part = prev_chunk[-cover_content:] if len(prev_chunk) > cover_content else prev_chunk
                            curr_chunk = cover_part + '\n\n' + para
                            curr_len = len(enc.encode(cover_part)) + 2 + para_len
                        else:
                            curr_chunk = para
                            curr_len = para_len
        
        if curr_chunk:
            chunk_text.append(curr_chunk)
        
        return chunk_text
    
    @classmethod
    def split_large_table(cls, table_text: str, token_len: int) -> List[str]:
        """
        æ™ºèƒ½åˆ†å‰²å¤§è¡¨æ ¼ï¼Œä¿ç•™è¡¨å¤´å’Œæ ‡é¢˜
        ç¡®ä¿æ¯ä¸ªchunkä¸è¶…è¿‡tokené™åˆ¶
        """
        chunks = []
        
        # æå–è¡¨æ ¼æ ‡é¢˜
        title_match = re.search(r'={60,}\n(ã€.*?ã€‘)\n={60,}', table_text)
        title = title_match.group(0) if title_match else ''
        
        # æå–è¡¨å¤´
        header_match = re.search(r'ã€è¡¨å¤´ã€‘\n(.*?)\n-{60,}', table_text, re.DOTALL)
        header = ''
        if header_match:
            header = f"ã€è¡¨å¤´ã€‘\n{header_match.group(1)}\n{'-'*80}"
        
        # æå–æ•°æ®è¡Œ
        data_match = re.search(r'ã€è¡¨æ ¼æ•°æ®ã€‘\n(.*?)(?=\n={60,}|\Z)', table_text, re.DOTALL)
        if not data_match:
            # å¦‚æœæ— æ³•è§£æï¼Œç›´æ¥æŒ‰è¡Œåˆ†å‰²
            lines = table_text.split('\n')
            curr_chunk = ''
            curr_len = 0
            
            # ä½¿ç”¨æ›´ä¿å®ˆçš„tokené™åˆ¶
            safe_limit = min(token_len, 600)
            
            for line in lines:
                line_len = len(enc.encode(line + '\n'))
                if curr_len + line_len <= safe_limit:
                    curr_chunk += line + '\n'
                    curr_len += line_len
                else:
                    if curr_chunk:
                        chunks.append(curr_chunk)
                    curr_chunk = line + '\n'
                    curr_len = line_len
            
            if curr_chunk:
                chunks.append(curr_chunk)
            
            return chunks if chunks else [table_text[:token_len*3]]
        
        data_lines = data_match.group(1).strip().split('\n')
        
        # è®¡ç®—è¡¨å¤´å’Œæ ‡é¢˜çš„tokené•¿åº¦
        header_len = len(enc.encode(title + '\n' + header))
        
        # æ›´ä¿å®ˆçš„å¯ç”¨é•¿åº¦è®¡ç®—ï¼Œç¡®ä¿ä¸è¶…è¿‡æ•´ä½“é™åˆ¶
        available_len = min(token_len - header_len - 200, 400)  # æœ€å¤š400 tokençš„æ•°æ®
        
        if available_len < 100:
            # å¦‚æœè¡¨å¤´å¤ªé•¿ï¼Œåªèƒ½ç®€åŒ–æˆ–æ”¾å¼ƒè¡¨å¤´
            available_len = min(token_len - 100, 500)
            title = ''
            header = ''
        
        # åˆ†æ‰¹å¤„ç†æ•°æ®è¡Œ
        curr_chunk_lines = []
        curr_data_len = 0
        
        for line in data_lines:
            line = line.strip()
            if not line:
                continue
            
            line_len = len(enc.encode(line + '\n'))
            
            # å¦‚æœå•è¡Œå°±è¶…è¿‡å¯ç”¨é•¿åº¦ï¼Œéœ€è¦æˆªæ–­
            if line_len > available_len:
                # å…ˆä¿å­˜å½“å‰ç§¯ç´¯çš„è¡Œ
                if curr_chunk_lines:
                    chunk = title + '\n' + header + '\nã€è¡¨æ ¼æ•°æ®ã€‘\n' + '\n'.join(curr_chunk_lines) + '\n' + '='*60
                    chunks.append(chunk)
                    curr_chunk_lines = []
                    curr_data_len = 0
                
                # æˆªæ–­è¶…é•¿è¡Œ
                truncated_line = line[:available_len * 3]  # ç²—ç•¥ä¼°ç®—
                curr_chunk_lines = [truncated_line]
                curr_data_len = len(enc.encode(truncated_line))
                continue
            
            if curr_data_len + line_len <= available_len:
                curr_chunk_lines.append(line)
                curr_data_len += line_len
            else:
                # ä¿å­˜å½“å‰chunk
                if curr_chunk_lines:
                    chunk = title + '\n' + header + '\nã€è¡¨æ ¼æ•°æ®ã€‘\n' + '\n'.join(curr_chunk_lines) + '\n' + '='*60
                    chunks.append(chunk)
                
                # å¼€å§‹æ–°chunk
                curr_chunk_lines = [line]
                curr_data_len = line_len
        
        # ä¿å­˜æœ€åä¸€ä¸ªchunk
        if curr_chunk_lines:
            chunk = title + '\n' + header + '\nã€è¡¨æ ¼æ•°æ®ã€‘\n' + '\n'.join(curr_chunk_lines) + '\n' + '='*60
            chunks.append(chunk)
        
        return chunks if chunks else [table_text[:token_len*3]]

    @classmethod
    def split_long_paragraph(cls, para: str, token_len: int) -> List[str]:
        """
        åˆ†å‰²è¶…é•¿æ®µè½ï¼Œå°½é‡åœ¨å¥å­è¾¹ç•Œå¤„åˆ†å‰²
        ç‰¹åˆ«å¤„ç†HTMLè¡¨æ ¼
        """
        # æ£€æµ‹æ˜¯å¦ä¸ºHTMLè¡¨æ ¼
        if para.strip().startswith('<table'):
            # æŒ‰è¡¨æ ¼è¡Œåˆ†å‰²
            rows = re.split(r'(<tr>|</tr>)', para)
            chunks = []
            curr_chunk = ''
            curr_len = 0
            
            for row in rows:
                if not row or row in ['<tr>', '</tr>']:
                    curr_chunk += row
                    continue
                
                row_len = len(enc.encode(row))
                
                # å¦‚æœå•è¡Œå°±è¶…è¿‡é™åˆ¶ï¼Œå¼ºåˆ¶æˆªæ–­
                if row_len > token_len:
                    if curr_chunk:
                        chunks.append(curr_chunk)
                        curr_chunk = ''
                        curr_len = 0
                    # è¶…é•¿è¡ŒæŒ‰å­—ç¬¦åˆ†å‰²
                    for i in range(0, len(row), token_len * 3):  # ç²—ç•¥ä¼°ç®—
                        sub_row = row[i:i + token_len * 3]
                        if len(enc.encode(sub_row)) <= token_len:
                            chunks.append(sub_row)
                        else:
                            # ç»§ç»­ç»†åˆ†
                            chunks.append(sub_row[:token_len * 2])
                    continue
                
                if curr_len + row_len <= token_len:
                    curr_chunk += row
                    curr_len += row_len
                else:
                    if curr_chunk:
                        chunks.append(curr_chunk)
                    curr_chunk = row
                    curr_len = row_len
            
            if curr_chunk:
                chunks.append(curr_chunk)
            
            return chunks if chunks else [para[:token_len * 3]]
        
        # æ™®é€šæ–‡æœ¬æŒ‰å¥å­åˆ†å‰²ï¼ˆä¸­è‹±æ–‡ï¼‰
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\.!?])', para)
        
        chunks = []
        curr_chunk = ''
        curr_len = 0
        
        for i in range(0, len(sentences), 2):
            if i + 1 < len(sentences):
                sentence = sentences[i] + sentences[i + 1]
            else:
                sentence = sentences[i]
            
            sentence = sentence.strip()
            if not sentence:
                continue
            
            sent_len = len(enc.encode(sentence))
            
            # å¦‚æœå•ä¸ªå¥å­å°±è¶…è¿‡é™åˆ¶ï¼ŒæŒ‰å­—ç¬¦å¼ºåˆ¶åˆ†å‰²
            if sent_len > token_len:
                if curr_chunk:
                    chunks.append(curr_chunk)
                    curr_chunk = ''
                    curr_len = 0
                # å¼ºåˆ¶æŒ‰token_lenåˆ†å‰²
                for i in range(0, len(sentence), token_len * 3):
                    chunks.append(sentence[i:i + token_len * 3])
                continue
            
            if curr_len + sent_len <= token_len:
                curr_chunk += sentence
                curr_len += sent_len
            else:
                if curr_chunk:
                    chunks.append(curr_chunk)
                curr_chunk = sentence
                curr_len = sent_len
        
        if curr_chunk:
            chunks.append(curr_chunk)
        
        return chunks if chunks else [para[:token_len * 3]]


class QueryParser:
    """
    æŸ¥è¯¢è§£æå™¨ï¼Œæå–æŸ¥è¯¢ä¸­çš„å…³é”®ä¿¡æ¯
    """
    
    @staticmethod
    def extract_entities(query: str) -> Dict[str, List[str]]:
        """
        ä»æŸ¥è¯¢ä¸­æå–å®ä½“
        """
        entities = {
            'years': [],
            'standards': [],
            'companies': [],
            'numbers': [],
            'locations': []
        }
        
        # æå–å¹´ä»½
        years = re.findall(r'\b(19|20)\d{2}\b', query)
        entities['years'] = years
        
        # æå–æ ‡å‡†å·ï¼ˆå¦‚GB/T 12345-2023, IEEE 1474.1ç­‰ï¼‰
        standards = re.findall(r'\b[A-Z]{2,}[\/\s]*[A-Z]*\s*\d+[\.\-]\d+[\-\d]*\b', query)
        entities['standards'] = standards
        
        # æå–æ•°å­—
        numbers = re.findall(r'\d+', query)
        entities['numbers'] = numbers
        
        # æå–åœ°ç‚¹ï¼ˆç®€å•çš„ä¸­æ–‡åœ°åæ£€æµ‹ï¼‰
        locations = re.findall(r'(åŒ—äº¬|ä¸Šæµ·|å¹¿å·|æ·±åœ³|å—äº¬|æ­å·|æ­¦æ±‰|æˆéƒ½|é‡åº†|å¤©æ´¥|[\u4e00-\u9fa5]{2,}å¸‚|[\u4e00-\u9fa5]{2,}çœ)', query)
        entities['locations'] = locations
        
        return entities
    
    @staticmethod
    def is_comparison_query(query: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºå¯¹æ¯”ç±»æŸ¥è¯¢
        """
        comparison_keywords = ['å¯¹æ¯”', 'æ¯”è¾ƒ', 'ä¸åŒ', 'å·®å¼‚', 'åŒºåˆ«', 'æ¼”å˜', 'å˜åŒ–', 'vs', 'versus']
        return any(keyword in query.lower() for keyword in comparison_keywords)
    
    @staticmethod
    def is_ranking_query(query: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºæ’åç±»æŸ¥è¯¢
        """
        ranking_keywords = ['æ’å', 'æ’ç¬¬å‡ ', 'æ’è¡Œ', 'ç¬¬å‡ ', 'åæ¬¡']
        return any(keyword in query for keyword in ranking_keywords)
    
    @staticmethod
    def is_listing_query(query: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºåˆ—ä¸¾ç±»æŸ¥è¯¢
        """
        listing_keywords = ['åˆ—å‡º', 'æ‰€æœ‰', 'å…¨éƒ¨', 'å®Œæ•´', 'å“ªäº›', 'åˆ†åˆ«']
        return any(keyword in query for keyword in listing_keywords)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    reader = EnhancedReadFiles('../data')
    docs = reader.get_content(max_token_len=400, cover_content=50)
    print(f"Total chunks: {len(docs)}")
    print(f"First chunk preview: {docs[0][:200]}...")

