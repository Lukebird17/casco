#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
@File    :   demo_enhanced.py
@Time    :   2025/11/16
@Desc    :   å¢å¼ºç‰ˆæ™ºèƒ½ä½“ä½¿ç”¨ç¤ºä¾‹
'''

from VectorBase import VectorStore
from LLM import OpenAIChat
from Embeddings import OpenAIEmbedding
from enhanced_agent import EnhancedRAGAgent
import json


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºå¢å¼ºç‰ˆæ™ºèƒ½ä½“çš„ä½¿ç”¨"""
    
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘         å¢å¼ºç‰ˆæ™ºèƒ½ä½“ Demo                            â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
    
    # 1. åŠ è½½å‘é‡æ•°æ®åº“
    print("ğŸ“‚ åŠ è½½å‘é‡æ•°æ®åº“...")
    vector_store = VectorStore()
    vector_store.load_vector('./storage_demo')
    print(f"âœ… åŠ è½½å®Œæˆï¼Œæ–‡æ¡£æ•°é‡: {len(vector_store.document)}\n")
    
    # 2. åˆå§‹åŒ–æ¨¡å‹
    print("ğŸ¤– åˆå§‹åŒ–æ¨¡å‹...")
    embedding = OpenAIEmbedding()
    llm = OpenAIChat()
    print("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ\n")
    
    # 3. åˆ›å»ºå¢å¼ºç‰ˆæ™ºèƒ½ä½“
    print("ğŸš€ åˆ›å»ºå¢å¼ºç‰ˆæ™ºèƒ½ä½“...")
    agent = EnhancedRAGAgent(
        vector_store=vector_store,
        llm=llm,
        embedding=embedding,
        enable_tracking=True  # å¯ç”¨Tokenè¿½è¸ª
    )
    print("âœ… æ™ºèƒ½ä½“åˆ›å»ºå®Œæˆ\n")
    
    # 4. æµ‹è¯•é—®é¢˜
    test_questions = [
        "å—äº¬åœ°é“S7å·çº¿çš„è¿è¥é‡Œç¨‹ï¼Œåœ¨æ±Ÿè‹çœå†…å·²è¿è¥åœ°é“é•¿åº¦ä¸­æ’ç¬¬å‡ ï¼Ÿ",
        # å¯ä»¥æ·»åŠ æ›´å¤šæµ‹è¯•é—®é¢˜
    ]
    
    print("="*60)
    print("å¼€å§‹å¤„ç†é—®é¢˜\n")
    
    results = []
    for i, question in enumerate(test_questions, 1):
        print(f"\n{'='*60}")
        print(f"é—®é¢˜ {i}: {question}")
        print(f"{'='*60}\n")
        
        # 5. æ‰§è¡ŒæŸ¥è¯¢ï¼ˆä½¿ç”¨å®Œæ•´å¢å¼ºåŠŸèƒ½ï¼‰
        result = agent.query_with_full_features(question)
        
        # 6. æ˜¾ç¤ºç­”æ¡ˆ
        print(f"\nâœ… ç­”æ¡ˆ:")
        print(f"{result['answer']}\n")
        
        # 7. æ˜¾ç¤ºæ¨ç†é“¾ï¼ˆå¯é€‰ï¼‰
        if result.get('reasoning_chain'):
            print("\n" + "="*60)
            print("æ¨ç†è¿‡ç¨‹:")
            print("="*60)
            print(result['reasoning_chain'].format_chain(detailed=False))
        
        # 8. æ˜¾ç¤ºTokenä½¿ç”¨æƒ…å†µ
        if result.get('token_usage'):
            print(f"\nğŸ“Š æœ¬æ¬¡æŸ¥è¯¢Tokenæ¶ˆè€—:")
            print(f"  â€¢ æ€»è®¡: {result['token_usage']['total_tokens']:,} tokens")
        
        # 9. æ ¼å¼åŒ–è¾“å‡ºï¼ˆç¬¦åˆç«èµ›è¦æ±‚ï¼‰
        formatted_output = agent.format_output(result, include_reasoning=False)
        results.append(formatted_output)
        
        print("\n" + "="*60)
    
    # 10. ä¿å­˜ç»“æœ
    print("\nğŸ’¾ ä¿å­˜ç»“æœ...")
    with open('enhanced_demo_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print("âœ… ç»“æœå·²ä¿å­˜åˆ°: enhanced_demo_results.json\n")
    
    # 11. æ˜¾ç¤ºæ€§èƒ½æŠ¥å‘Š
    print("\n" + "="*60)
    print("æ€§èƒ½æŠ¥å‘Š")
    print("="*60)
    print(agent.get_performance_report())
    
    # 12. ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    print("\nğŸ’¾ ä¿å­˜è¯¦ç»†æŠ¥å‘Š...")
    agent.save_reports(output_dir='enhanced_reports')
    
    print("\nâœ… Demoå®Œæˆï¼")


def demo_advanced_features():
    """æ¼”ç¤ºé«˜çº§åŠŸèƒ½"""
    print("\n\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘         é«˜çº§åŠŸèƒ½æ¼”ç¤º                                  â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
    
    # æ¼”ç¤º1ï¼šTokenä¼˜åŒ–
    print("ã€åŠŸèƒ½1ï¼šTokenä¼˜åŒ–ã€‘")
    from token_tracker import TokenTracker
    
    tracker = TokenTracker()
    long_text = "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„æ–‡æœ¬ã€‚\n" * 500
    print(f"åŸå§‹æ–‡æœ¬: {len(long_text)} å­—ç¬¦, {tracker.count_tokens(long_text)} tokens")
    
    optimized = tracker.optimize_context(long_text, max_tokens=200)
    print(f"ä¼˜åŒ–å: {len(optimized)} å­—ç¬¦, {tracker.count_tokens(optimized)} tokens")
    print(f"èŠ‚çœ: {len(long_text) - len(optimized)} å­—ç¬¦\n")
    
    # æ¼”ç¤º2ï¼šæ¨ç†é“¾
    print("ã€åŠŸèƒ½2ï¼šæ¨ç†é“¾è®°å½•ã€‘")
    from reasoning_chain import ReasoningChain
    
    chain = ReasoningChain("æµ‹è¯•é—®é¢˜")
    chain.add_analysis_step("åˆ†æé—®é¢˜ç±»å‹")
    chain.add_retrieval_step("æ£€ç´¢ç›¸å…³æ–‡æ¡£")
    chain.add_inference_step("è¿›è¡Œé€»è¾‘æ¨ç†", confidence=0.9)
    chain.add_conclusion_step("å¾—å‡ºç»“è®º")
    
    print(chain.format_compact())
    print()
    
    # æ¼”ç¤º3ï¼šè¡¨æ ¼æå–
    print("ã€åŠŸèƒ½3ï¼šè¡¨æ ¼æå–ã€‘")
    from advanced_document_processor import AdvancedTableExtractor
    
    extractor = AdvancedTableExtractor()
    print("è¡¨æ ¼æå–å™¨å·²åˆå§‹åŒ–")
    print("æ”¯æŒpdfplumberå’ŒPyMuPDFä¸¤ç§æ–¹æ³•\n")
    
    # æ¼”ç¤º4ï¼šç‰ˆæœ¬å¯¹æ¯”
    print("ã€åŠŸèƒ½4ï¼šç‰ˆæœ¬å¯¹æ¯”ã€‘")
    from advanced_document_processor import VersionComparator
    
    comparator = VersionComparator()
    doc1 = "ç‰ˆæœ¬1çš„å†…å®¹\nåŒ…å«åŠŸèƒ½A\nåŒ…å«åŠŸèƒ½B"
    doc2 = "ç‰ˆæœ¬2çš„å†…å®¹\nåŒ…å«åŠŸèƒ½A\nåŒ…å«åŠŸèƒ½B\næ–°å¢åŠŸèƒ½C"
    
    comparison = comparator.compare_documents(doc1, doc2)
    print(f"ç›¸ä¼¼åº¦: {comparison['similarity_ratio']:.1%}")
    print(f"æ–°å¢: {comparison['added_lines']}è¡Œ")
    print(f"åˆ é™¤: {comparison['removed_lines']}è¡Œ\n")


if __name__ == "__main__":
    try:
        # è¿è¡Œä¸»Demo
        main()
        
        # è¿è¡Œé«˜çº§åŠŸèƒ½æ¼”ç¤º
        demo_advanced_features()
        
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        print("\næç¤ºï¼šè¯·ç¡®ä¿:")
        print("1. .envæ–‡ä»¶é…ç½®æ­£ç¡®")
        print("2. å‘é‡æ•°æ®åº“å·²æ„å»º (./storage_demo)")
        print("3. æ‰€æœ‰ä¾èµ–å·²å®‰è£…")
        import traceback
        traceback.print_exc()

