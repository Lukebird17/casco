#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
@File    :   LLM.py
@Time    :   2025/11/10
@Ref   :   ä¸è¦è‘±å§œè’œ
'''
import os
from typing import Dict, List, Optional, Tuple, Union
from openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain_core.messages.human import HumanMessage
from langchain_core.callbacks import StdOutCallbackHandler
from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv())

RAG_PROMPT_TEMPLATE="""
ä½¿ç”¨ä»¥ä¸Šä¸‹æ–‡æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ã€‚æ€»æ˜¯ä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚
é—®é¢˜: {question}
å¯å‚è€ƒçš„ä¸Šä¸‹æ–‡ï¼š
Â·Â·Â·
{context}
Â·Â·Â·
å¦‚æœç»™å®šçš„ä¸Šä¸‹æ–‡æ— æ³•è®©ä½ åšå‡ºå›ç­”ï¼Œè¯·å›ç­”æ•°æ®åº“ä¸­æ²¡æœ‰è¿™ä¸ªå†…å®¹ï¼Œä½ ä¸çŸ¥é“ã€‚
æœ‰ç”¨çš„å›ç­”:
"""


class BaseModel:
    def __init__(self, model) -> None:
        self.model = model

    def chat(self, prompt: str, history: List[dict], content: str) -> str:
        pass

    def load_model(self):
        pass

class OpenAIChat(BaseModel):
    def __init__(self, model: str = "deepseek") -> None:
        self.model = model

    def chat(self, prompt: str, history: List[dict], content: str) -> str:
        # é™åˆ¶ context é•¿åº¦ï¼Œé¿å…è¶…è¿‡ token é™åˆ¶
        max_context_length = 30000  # å­—ç¬¦æ•°é™åˆ¶
        if len(content) > max_context_length:
            print(f"âš ï¸ ä¸Šä¸‹æ–‡è¿‡é•¿ ({len(content)} å­—ç¬¦)ï¼Œæˆªæ–­åˆ° {max_context_length} å­—ç¬¦")
            content = content[:max_context_length] + "\n...(å†…å®¹å·²æˆªæ–­)"
        
        llm = ChatOpenAI(
            model_name= os.getenv("CLOUD_MODEL"),
            openai_api_key=os.getenv("CLOUD_API_KEY"),
            openai_api_base=os.getenv("CLOUD_BASE_URL"),
            callbacks=[StdOutCallbackHandler()],  # å®æ—¶æ‰“å°ç”Ÿæˆå†…å®¹
            temperature=0.7,
            max_tokens=4096,  # é™åˆ¶è¾“å‡ºé•¿åº¦
            timeout=120,  # è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º120ç§’
            max_retries=2  # è‡ªåŠ¨é‡è¯•2æ¬¡
        )
        
        # åˆ›å»ºä¸€ä¸ªèŠå¤©æ¶ˆæ¯
        history.append({'role': 'user', 'content': RAG_PROMPT_TEMPLATE.format(question=prompt, context=content)})
        messages = [HumanMessage(content=message['content']) for message in history]
        
        # æ·»åŠ é”™è¯¯å¤„ç†
        try:
            response = llm.invoke(messages)
            return response.content
        except Exception as e:
            error_msg = str(e)
            print(f"âŒ API è°ƒç”¨å¤±è´¥: {error_msg}")
            # å¦‚æœæ˜¯ token é™åˆ¶é—®é¢˜ï¼Œå°è¯•æ›´çŸ­çš„ä¸Šä¸‹æ–‡
            if "token" in error_msg.lower() or "length" in error_msg.lower():
                print("ğŸ”„ æ£€æµ‹åˆ° token é™åˆ¶é—®é¢˜ï¼Œå°è¯•ä½¿ç”¨æ›´çŸ­çš„ä¸Šä¸‹æ–‡...")
                content = content[:15000] + "\n...(å†…å®¹å·²å¤§å¹…æˆªæ–­)"
                history[-1]['content'] = RAG_PROMPT_TEMPLATE.format(question=prompt, context=content)
                messages = [HumanMessage(content=message['content']) for message in history]
                response = llm.invoke(messages)
                return response.content
            else:
                raise


if __name__ == "__main__":
    model = OpenAIChat()
    response = model.chat("ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ", [], "ä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬ã€‚")
    print(response)
