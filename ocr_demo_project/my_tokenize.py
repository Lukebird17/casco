# build_storage_bge.py
import os
import re
from pathlib import Path
from typing import List
from tqdm import tqdm
from typing import List, Dict, Any, Tuple
# å¯¼å…¥ç°æœ‰æ¨¡å—å’Œæ–°å†™çš„ BGE æ¨¡å—
from VectorBase import VectorStore
from my_BGE_embedding import BGEEmbedding
from LLM import OpenAIChat
SUPPORTED_LANGS = ['zh', 'en', 'es', 'fr', 'de', 'ja', 'ko']

# æ–°å¢ï¼šè¯­è¨€ä»£ç åˆ°åç§°çš„æ˜ å°„
LANGUAGE_NAMES = {
    'zh': 'ä¸­æ–‡',
    'en': 'è‹±è¯­',
    'es': 'è¥¿ç­ç‰™è¯­',
    'fr': 'æ³•è¯­',
    'de': 'å¾·è¯­',
    'ja': 'æ—¥è¯­',
    'ko': 'éŸ©è¯­',
    # å¦‚æœéœ€è¦ï¼Œå¯ä»¥æ·»åŠ æ›´å¤šè¯­è¨€
}

def predict_document_language(file_path: Path, llm_instance: OpenAIChat) -> str:
    """
    é€šè¿‡æ–‡ä»¶åå’Œæ–‡æ¡£å‰100å­—ç¬¦ï¼Œç»“åˆLLM APIæ¨æ–­æ–‡æ¡£è¯­è¨€ã€‚
    
    Args:
        file_path: æ–‡æ¡£è·¯å¾„ã€‚
        llm_instance: ç”¨äºè°ƒç”¨è¯­è¨€æ£€æµ‹APIçš„LLMå®ä¾‹ (e.g., OpenAIChat)ã€‚
        
    Returns:
        æ¨æ–­å‡ºçš„è¯­è¨€ä»£ç  ('zh', 'en', ç­‰)ã€‚
    """
    file_name = file_path.name
    # è¾…åŠ©å‡½æ•°ï¼šæ ¼å¼åŒ–è¾“å‡ºç»“æœ
    def format_output(lang_code, reason):
        lang_name = LANGUAGE_NAMES.get(lang_code, f"æœªçŸ¥ ({lang_code})")
        print(f"âœ… æ–‡ä»¶ '{file_name}' è¯­è¨€è¯†åˆ«æˆåŠŸ: {lang_name} ({lang_code}) - æ¥æº: {reason}")
        return lang_code
    
    # æ£€æŸ¥ LLM å®ä¾‹æ˜¯å¦æœ‰æ•ˆ (è§£å†³ NoneType é”™è¯¯)
    if llm_instance is None:
        return 'zh' # LLM å¤±è´¥æ—¶ä½¿ç”¨é»˜è®¤è¯­è¨€

    # 2. è¯»å–æ–‡æ¡£å†…å®¹ (ç”¨äºLLMæ¨æ–­)
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read(100) # åªè¯»å–å‰100ä¸ªå­—ç¬¦
    except Exception:
        content = ""

    # 3. LLMæ¨æ–­
    # æ£€æŸ¥ llm_instance æ˜¯å¦æœ‰æ­£ç¡®çš„å®Œæˆæ–¹æ³•
    if not hasattr(llm_instance, 'get_completion'):
        print(f"âš ï¸ LLMè¯­è¨€æ¨æ–­å¤±è´¥: LLMå®ä¾‹ç¼ºå°‘ 'get_completion' æ–¹æ³•ï¼Œä½¿ç”¨é»˜è®¤è¯­è¨€ 'zh'")
        return 'zh'

    # æ„å»ºPrompt
    prompt = f"è¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹æ–‡ä»¶å†…å®¹ç‰‡æ®µï¼Œæ¨æ–­å…¶ä¸»è¦è¯­è¨€ã€‚æ–‡ä»¶åä¸º {file_name}ï¼Œå†…å®¹ä¸ºï¼š'{content}'ã€‚è¯·åªè¾“å‡ºè¯­è¨€ä»£ç ï¼Œä¾‹å¦‚ 'zh', 'en', 'ja'ã€‚"
    
    try:
        # **é‡è¦ä¿®æ”¹:** å°† llm_instance.invoke æ”¹ä¸º llm_instance.get_completion
        result = llm_instance.get_completion(prompt).strip().lower() 
        code_match = re.search(r'\b(zh|en|es|fr|de|ja|ko)\b', result)

        if code_match:
            final_code = code_match.group(1)
            return format_output(final_code, "LLMæ¨æ–­")
        else:
            print(f"âš ï¸ æ–‡ä»¶ '{file_name}': LLMæ¨æ–­è¿”å›äº†ä¸æ”¯æŒæˆ–ä¸æ¸…æ™°çš„ç»“æœ '{result}'ï¼Œä½¿ç”¨é»˜è®¤è¯­è¨€ 'ä¸­æ–‡ (zh)'")
            return 'zh'
            
    except Exception as e:
        # æ•è· LLM API è°ƒç”¨å¤±è´¥çš„å¼‚å¸¸ (åŒ…æ‹¬ç½‘ç»œé”™è¯¯ã€è®¤è¯é”™è¯¯ç­‰)
        # æ­¤æ—¶çš„ LLM å®ä¾‹ä¸æ˜¯ Noneï¼Œè€Œæ˜¯è°ƒç”¨å¤±è´¥
        print(f"âš ï¸ æ–‡ä»¶ '{file_name}': LLMè¯­è¨€æ¨æ–­å¤±è´¥ (API è°ƒç”¨é”™è¯¯: {e})ï¼Œä½¿ç”¨é»˜è®¤è¯­è¨€ 'ä¸­æ–‡ (zh)'")
        return 'zh'

def recursive_split_text(text: str, max_len: int = 500, overlap: int = 50) -> List[str]:
    """
    æ™ºèƒ½åˆ†æ®µå‡½æ•°ï¼šä¼˜å…ˆåœ¨æ®µè½å’Œå¥å­ç»“æŸç¬¦å¤„åˆ‡åˆ†ï¼Œä¿è¯å¥å­å®Œæ•´æ€§
    """
    # 1. å¦‚æœæ–‡æœ¬æœ¬èº«å¾ˆçŸ­ï¼Œç›´æ¥è¿”å›
    if len(text) <= max_len:
        return [text]
    
    # 2. å®šä¹‰åˆ†éš”ç¬¦ä¼˜å…ˆçº§ï¼šåŒæ¢è¡Œ(æ®µè½) > å•æ¢è¡Œ > ä¸­æ–‡å¥å·ç­‰ > è‹±æ–‡å¥å· > é€—å· > å¼ºåˆ¶åˆ‡åˆ†
    separators = ["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", "ï¼›", ";", "ï¼Œ", ","]
    
    chunks = []
    
    # å°è¯•æ‰¾åˆ°æœ€ä½³åˆ‡åˆ†ç‚¹
    for sep in separators:
        if sep in text:
            # æŒ‰åˆ†éš”ç¬¦åˆæ­¥åˆ‡åˆ†
            splits = text.split(sep)
            current_chunk = ""
            
            for split in splits:
                # æ¢å¤åˆ†éš”ç¬¦ï¼ˆé™¤äº†æ¢è¡Œç¬¦ï¼Œé€šå¸¸å¥å·éœ€è¦ä¿ç•™ï¼‰
                token = sep if sep not in ["\n\n", "\n"] else " "
                segment = split + token
                
                if len(current_chunk) + len(segment) < max_len:
                    current_chunk += segment
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    # å¤„ç†é‡å ï¼šå–ä¸Šä¸€ä¸ª chunk çš„å overlap ä¸ªå­—ç¬¦ä½œä¸ºä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
                    current_chunk = segment
            
            if current_chunk:
                chunks.append(current_chunk.strip())
                
            # å¦‚æœåˆ‡åˆ†æˆåŠŸï¼ˆäº§ç”Ÿäº†å¤šä¸ªç‰‡æ®µï¼‰ï¼Œåˆ™ç»“æŸå½“å‰å±‚çº§çš„åˆ‡åˆ†
            if len(chunks) > 0:
                return chunks
    
    # 3. å¦‚æœæ‰€æœ‰åˆ†éš”ç¬¦éƒ½å¤±æ•ˆï¼ˆæ¯”å¦‚ä¸€æ•´æ®µæ²¡æœ‰æ ‡ç‚¹ï¼‰ï¼Œåªèƒ½å¼ºåˆ¶æŒ‰å­—ç¬¦åˆ‡åˆ†
    return [text[i:i+max_len] for i in range(0, len(text), max_len-overlap)]

# â— ä¿®æ”¹å‡½æ•°ç­¾åå’Œè¿”å›ç»“æ„
def process_markdown_files(folder_path: str, llm_instance: OpenAIChat, max_len: int = 400) -> List[Dict[str, Any]]:
    """
    è¯»å–æ–‡ä»¶å¤¹ï¼Œå¤„ç†æ–‡æ¡£ï¼Œä¿ç•™æ ‡é¢˜ä¿¡æ¯
    """
    all_chunks: List[Dict[str, Any]] = [] # å­˜å‚¨ List[Dict]
    md_files = list(Path(folder_path).rglob('*.md'))
    
    print(f"ğŸ“‚ æ‰«æåˆ° {len(md_files)} ä¸ª Markdown æ–‡ä»¶")
    
    for file_path in tqdm(list(Path(folder_path).glob('*.md')), desc="å¤„ç†æ–‡æ¡£åˆ‡åˆ†"):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            file_name = file_path.stem
            
            # â— æ–°å¢ï¼šæ¨æ–­æ–‡ä»¶è¯­è¨€
            file_lang = predict_document_language(file_path, llm_instance)

            # --- æ ¸å¿ƒç­–ç•¥ï¼šæ™ºèƒ½åˆ†æ®µ ---
            raw_chunks = recursive_split_text(content, max_len=max_len, overlap=100)
            
            # --- æ ¸å¿ƒç­–ç•¥ï¼šæ³¨å…¥å…ƒæ•°æ®ï¼ˆå†…å®¹ + è¯­è¨€æ ‡ç­¾ï¼‰ ---
            for chunk in raw_chunks:
                if len(chunk.strip()) < 10:
                    continue
                # å­˜å‚¨ä¸ºå­—å…¸ç»“æ„
                all_chunks.append({
                    'content': f"ã€Š{file_name}ã€‹\n{chunk}", # ä¿ç•™æ–‡ä»¶åä½œä¸ºä¸Šä¸‹æ–‡
                    'lang': file_lang # æ³¨å…¥è¯­è¨€æ ‡ç­¾
                })
                
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
            
    return all_chunks

def main():
    # 1. é…ç½®è·¯å¾„
    INPUT_FOLDER = 'output_paddle_table/' #'output_paddle/'  # ä½ çš„ OCR ç»“æœæ–‡ä»¶å¤¹
    STORAGE_PATH = './trial_bge'#'./storage_bge'   # æ–°çš„å­˜å‚¨è·¯å¾„
    
    # 2. å¤„ç†æ–‡æ¡£ä¸åˆ‡åˆ†
    print("ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£...")
    llm = OpenAIChat()
    documents = process_markdown_files(INPUT_FOLDER, llm)
    print(f"ğŸ“Š æ–‡æ¡£å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(documents)} ä¸ªå‘é‡ç‰‡æ®µ")
    
    # 3. åˆå§‹åŒ– BGE æ¨¡å‹
    # æ³¨æ„ï¼šç¬¬ä¸€æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ (çº¦ 2GB)ï¼Œè¯·ä¿æŒç½‘ç»œé€šç•…
    embedding_model = BGEEmbedding()
    
    # 4. åˆ›å»ºå‘é‡åº“å¹¶ç”Ÿæˆå‘é‡
    print("âš¡ å¼€å§‹ç”Ÿæˆå‘é‡ (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    vector_store = VectorStore(documents)
    vector_store.get_vector(embedding_model)
    
    # 5. æŒä¹…åŒ–å­˜å‚¨
    vector_store.persist(STORAGE_PATH)
    print(f"âœ… æˆåŠŸï¼å‘é‡åº“å·²ä¿å­˜è‡³: {STORAGE_PATH}")
    print("æç¤ºï¼šè¯·åœ¨ demo_enhanced.py ä¸­ä¿®æ”¹ vector_store.load_vector('./storage_bge') ä»¥ä½¿ç”¨æ–°åº“")

if __name__ == "__main__":
    main()