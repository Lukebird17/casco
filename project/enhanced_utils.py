#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
@File    :   enhanced_utils.py
@Time    :   2025/11/15
@Desc    :   å¢å¼ºçš„æ–‡æ¡£å¤„ç†å·¥å…·ï¼Œæ”¯æŒè¡¨æ ¼æå–å’Œç»“æ„åŒ–ä¿¡æ¯ä¿ç•™
'''

import os
import re
from typing import Dict, List, Tuple
import json
from tqdm import tqdm
import tiktoken
import fitz  # PyMuPDF
from PIL import Image
import io

enc = tiktoken.get_encoding("cl100k_base")


class EnhancedReadFiles:
    """
    å¢å¼ºçš„æ–‡ä»¶è¯»å–ç±»ï¼Œæ”¯æŒè¡¨æ ¼æå–å’Œå…ƒæ•°æ®ä¿ç•™
    """
    
    # ç±»çº§åˆ«çš„OCRç³»ç»Ÿå®ä¾‹ï¼ˆå…¨å±€å…±äº«ï¼Œåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
    _ocr_system = None

    def __init__(self, path: str) -> None:
        self._path = path
        self.file_list = self.get_files()
        
        # å¦‚æœOCRç³»ç»Ÿè¿˜æ²¡åˆå§‹åŒ–ï¼Œåˆ™åˆå§‹åŒ–ä¸€æ¬¡
        if EnhancedReadFiles._ocr_system is None:
            print("ğŸš€ åˆå§‹åŒ–OCRç³»ç»Ÿï¼ˆå…¨å±€å•ä¾‹ï¼‰...")
            from simple_ocr_system import SimpleOCRSystem
            EnhancedReadFiles._ocr_system = SimpleOCRSystem()
            print("âœ… OCRç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")

    def get_files(self):
        file_list = []
        for filepath, dirnames, filenames in os.walk(self._path):
            for filename in filenames:
                if filename.endswith((".md", ".txt", ".pdf")):
                    file_list.append(os.path.join(filepath, filename))
        return file_list

    def get_content(self, max_token_len: int = 600, cover_content: int = 150):
        """
        è·å–æ‰€æœ‰æ–‡æ¡£å†…å®¹ï¼Œä¿ç•™å…ƒæ•°æ®
        """
        docs = []
        for file in tqdm(self.file_list, desc="Processing files"):
            try:
                # è¯»å–æ–‡ä»¶å†…å®¹å’Œå…ƒæ•°æ®
                content, metadata = self.read_file_content_with_metadata(file)
                
                # åˆ†å—å¹¶é™„åŠ å…ƒæ•°æ®
                chunk_content = self.get_chunk_with_metadata(
                    content, metadata, 
                    max_token_len=max_token_len, 
                    cover_content=cover_content
                )
                docs.extend(chunk_content)
            except Exception as e:
                print(f"Error processing {file}: {e}")
                continue
        
        return docs

    @classmethod
    def read_file_content_with_metadata(cls, file_path: str) -> Tuple[str, Dict]:
        """
        è¯»å–æ–‡ä»¶å†…å®¹å¹¶æå–å…ƒæ•°æ®
        """
        metadata = {
            'source': file_path,
            'filename': os.path.basename(file_path),
            'file_type': os.path.splitext(file_path)[1]
        }
        
        if file_path.endswith('.pdf'):
            content = cls.read_pdf_enhanced(file_path, metadata)
        elif file_path.endswith('.md'):
            content = cls.read_markdown(file_path)
        elif file_path.endswith('.txt'):
            content = cls.read_text(file_path)
        else:
            raise ValueError("Unsupported file type")
        
        return content, metadata

    @classmethod
    def read_pdf_enhanced(cls, file_path: str, metadata: Dict) -> str:
        """
        å¢å¼ºçš„PDFè¯»å–ï¼Œä½¿ç”¨PaddleOCRæ”¯æŒè¡¨æ ¼ã€å¤šè¯­è¨€ã€æ—‹è½¬çº æ­£ç­‰
        Args:
            file_path: PDFæ–‡ä»¶è·¯å¾„
            metadata: å…ƒæ•°æ®å­—å…¸
        Returns:
            æå–çš„æ–‡æœ¬
        """
        # ä½¿ç”¨ç±»çº§åˆ«çš„OCRç³»ç»Ÿå®ä¾‹ï¼ˆå·²ç»åˆå§‹åŒ–å¥½äº†ï¼‰
        ocr_system = cls._ocr_system
        
        # ä½¿ç”¨PPStructureV3è¿›è¡Œæ–‡æ¡£ç»“æ„åˆ†æ
        results = ocr_system.process_document(
            file_path,
            use_structure_analysis=True,  # ä½¿ç”¨ç»“æ„åˆ†æï¼ˆç‰ˆé¢+è¡¨æ ¼+OCRï¼‰
            extract_toc=True              # æå–ç›®å½•
        )
        
        # åˆå¹¶æ‰€æœ‰é¡µé¢çš„æ–‡æœ¬
        full_text = []
        metadata['total_pages'] = len(results['pages'])
        
        for page in results['pages']:
            full_text.append(f"\n{'='*50}\n[ç¬¬ {page['page_number']} é¡µ]\n{'='*50}\n")
            
            # æ·»åŠ è¡¨æ ¼ï¼ˆMarkdownæ ¼å¼ï¼‰
            if page['tables']:
                for table in page['tables']:
                    if table.get('markdown'):
                        full_text.append(f"\n[è¡¨æ ¼ {table['table_index']}]\n{table['markdown']}\n")
            
            # æ·»åŠ æ–‡æœ¬
            if page['text']:
                full_text.append(page['text'])
        
        return "\n".join(full_text)

    @classmethod
    def detect_tables_in_page(cls, page) -> List[str]:
        """
        æ£€æµ‹é¡µé¢ä¸­çš„è¡¨æ ¼å¹¶æå–
        ä½¿ç”¨ç®€å•çš„å¯å‘å¼æ–¹æ³•æ£€æµ‹è¡¨æ ¼ç»“æ„
        """
        tables = []
        text = page.get_text("text")
        lines = text.split('\n')
        
        # ç®€å•çš„è¡¨æ ¼æ£€æµ‹ï¼šè¿ç»­å¤šè¡ŒåŒ…å«å¤šä¸ªç©ºæ ¼åˆ†éš”çš„å†…å®¹
        potential_table = []
        in_table = False
        
        for line in lines:
            # æ£€æµ‹æ˜¯å¦å¯èƒ½æ˜¯è¡¨æ ¼è¡Œï¼ˆåŒ…å«å¤šä¸ªè¿ç»­ç©ºæ ¼æˆ–åˆ¶è¡¨ç¬¦ï¼‰
            if re.search(r'\s{2,}|\t', line) and len(line.split()) >= 2:
                potential_table.append(line)
                in_table = True
            elif in_table:
                # å¦‚æœè¡¨æ ¼ä¸­æ–­ï¼Œä¿å­˜å½“å‰è¡¨æ ¼
                if len(potential_table) >= 3:  # è‡³å°‘3è¡Œæ‰ç®—è¡¨æ ¼
                    tables.append('\n'.join(potential_table))
                potential_table = []
                in_table = False
        
        # ä¿å­˜æœ€åä¸€ä¸ªè¡¨æ ¼
        if len(potential_table) >= 3:
            tables.append('\n'.join(potential_table))
        
        return tables

    @classmethod
    def read_markdown(cls, file_path: str) -> str:
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.read()

    @classmethod
    def read_text(cls, file_path: str) -> str:
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.read()

    @classmethod
    def get_chunk_with_metadata(cls, text: str, metadata: Dict, 
                                max_token_len: int = 600, 
                                cover_content: int = 150) -> List[str]:
        """
        åˆ†å—å¹¶ä¿ç•™å…ƒæ•°æ®ä¿¡æ¯
        """
        chunks = cls.get_chunk(text, max_token_len, cover_content)
        
        # ä¸ºæ¯ä¸ªchunkæ·»åŠ å…ƒæ•°æ®æ ‡è®°
        chunks_with_metadata = []
        for i, chunk in enumerate(chunks):
            # åœ¨chunkå¼€å¤´æ·»åŠ æ¥æºä¿¡æ¯
            metadata_header = f"[æ¥æº: {metadata['filename']}]"
            chunk_with_meta = f"{metadata_header}\n{chunk}"
            chunks_with_metadata.append(chunk_with_meta)
        
        return chunks_with_metadata

    @classmethod
    def get_chunk(cls, text: str, max_token_len: int = 600, cover_content: int = 150) -> List[str]:
        """
        æ™ºèƒ½åˆ†å—ï¼Œä¿ç•™æ®µè½å®Œæ•´æ€§
        """
        chunk_text = []
        curr_len = 0
        curr_chunk = ''
        token_len = max_token_len - cover_content
        
        # æŒ‰æ®µè½åˆ†å‰²ï¼ˆä¿ç•™é¡µé¢æ ‡è®°ï¼‰
        paragraphs = re.split(r'\n\s*\n', text)
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
                
            para_len = len(enc.encode(para))
            
            # å¦‚æœå•ä¸ªæ®µè½è¶…é•¿ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
            if para_len > max_token_len:
                if curr_chunk:
                    chunk_text.append(curr_chunk)
                    curr_chunk = ''
                    curr_len = 0
                
                # åˆ†å‰²é•¿æ®µè½
                sub_chunks = cls.split_long_paragraph(para, token_len)
                chunk_text.extend(sub_chunks)
                
            elif curr_len + para_len + 2 <= token_len:
                if curr_chunk:
                    curr_chunk += '\n\n'
                    curr_len += 2
                curr_chunk += para
                curr_len += para_len
            else:
                if curr_chunk:
                    chunk_text.append(curr_chunk)
                
                # æ·»åŠ è¦†ç›–å†…å®¹
                if chunk_text and cover_content > 0:
                    prev_chunk = chunk_text[-1]
                    cover_part = prev_chunk[-cover_content:] if len(prev_chunk) > cover_content else prev_chunk
                    curr_chunk = cover_part + '\n\n' + para
                    curr_len = len(enc.encode(cover_part)) + 2 + para_len
                else:
                    curr_chunk = para
                    curr_len = para_len
        
        if curr_chunk:
            chunk_text.append(curr_chunk)
        
        return chunk_text

    @classmethod
    def split_long_paragraph(cls, para: str, token_len: int) -> List[str]:
        """
        åˆ†å‰²è¶…é•¿æ®µè½ï¼Œå°½é‡åœ¨å¥å­è¾¹ç•Œå¤„åˆ†å‰²
        """
        # æŒ‰å¥å­åˆ†å‰²ï¼ˆä¸­è‹±æ–‡ï¼‰
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\.!?])', para)
        
        chunks = []
        curr_chunk = ''
        curr_len = 0
        
        for i in range(0, len(sentences), 2):
            if i + 1 < len(sentences):
                sentence = sentences[i] + sentences[i + 1]
            else:
                sentence = sentences[i]
            
            sentence = sentence.strip()
            if not sentence:
                continue
            
            sent_len = len(enc.encode(sentence))
            
            if curr_len + sent_len <= token_len:
                curr_chunk += sentence
                curr_len += sent_len
            else:
                if curr_chunk:
                    chunks.append(curr_chunk)
                curr_chunk = sentence
                curr_len = sent_len
        
        if curr_chunk:
            chunks.append(curr_chunk)
        
        return chunks


class QueryParser:
    """
    æŸ¥è¯¢è§£æå™¨ï¼Œæå–æŸ¥è¯¢ä¸­çš„å…³é”®ä¿¡æ¯
    """
    
    @staticmethod
    def extract_entities(query: str) -> Dict[str, List[str]]:
        """
        ä»æŸ¥è¯¢ä¸­æå–å®ä½“
        """
        entities = {
            'years': [],
            'standards': [],
            'companies': [],
            'numbers': [],
            'locations': []
        }
        
        # æå–å¹´ä»½
        years = re.findall(r'\b(19|20)\d{2}\b', query)
        entities['years'] = years
        
        # æå–æ ‡å‡†å·ï¼ˆå¦‚GB/T 12345-2023, IEEE 1474.1ç­‰ï¼‰
        standards = re.findall(r'\b[A-Z]{2,}[\/\s]*[A-Z]*\s*\d+[\.\-]\d+[\-\d]*\b', query)
        entities['standards'] = standards
        
        # æå–æ•°å­—
        numbers = re.findall(r'\d+', query)
        entities['numbers'] = numbers
        
        # æå–åœ°ç‚¹ï¼ˆç®€å•çš„ä¸­æ–‡åœ°åæ£€æµ‹ï¼‰
        locations = re.findall(r'(åŒ—äº¬|ä¸Šæµ·|å¹¿å·|æ·±åœ³|å—äº¬|æ­å·|æ­¦æ±‰|æˆéƒ½|é‡åº†|å¤©æ´¥|[\u4e00-\u9fa5]{2,}å¸‚|[\u4e00-\u9fa5]{2,}çœ)', query)
        entities['locations'] = locations
        
        return entities
    
    @staticmethod
    def is_comparison_query(query: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºå¯¹æ¯”ç±»æŸ¥è¯¢
        """
        comparison_keywords = ['å¯¹æ¯”', 'æ¯”è¾ƒ', 'ä¸åŒ', 'å·®å¼‚', 'åŒºåˆ«', 'æ¼”å˜', 'å˜åŒ–', 'vs', 'versus']
        return any(keyword in query.lower() for keyword in comparison_keywords)
    
    @staticmethod
    def is_ranking_query(query: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºæ’åç±»æŸ¥è¯¢
        """
        ranking_keywords = ['æ’å', 'æ’ç¬¬å‡ ', 'æ’è¡Œ', 'ç¬¬å‡ ', 'åæ¬¡']
        return any(keyword in query for keyword in ranking_keywords)
    
    @staticmethod
    def is_listing_query(query: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºåˆ—ä¸¾ç±»æŸ¥è¯¢
        """
        listing_keywords = ['åˆ—å‡º', 'æ‰€æœ‰', 'å…¨éƒ¨', 'å®Œæ•´', 'å“ªäº›', 'åˆ†åˆ«']
        return any(keyword in query for keyword in listing_keywords)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    reader = EnhancedReadFiles('../data')
    docs = reader.get_content(max_token_len=400, cover_content=50)
    print(f"Total chunks: {len(docs)}")
    print(f"First chunk preview: {docs[0][:200]}...")

