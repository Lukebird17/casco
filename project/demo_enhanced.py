#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
@File    :   demo_enhanced.py
@Time    :   2025/11/16
@Desc    :   å¢å¼ºç‰ˆæ™ºèƒ½ä½“ä½¿ç”¨ç¤ºä¾‹
'''

from VectorBase import VectorStore
from LLM import OpenAIChat
from my_BGE_embedding import BGEEmbedding  # å¯¼å…¥ä½ åˆšå†™å¥½çš„BGEç±»
from enhanced_agent import EnhancedRAGAgent
import json
import os

RESULTS_FILE = "enhanced_demo_results.json"

def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºå¢å¼ºç‰ˆæ™ºèƒ½ä½“çš„ä½¿ç”¨"""
    
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘         å¢å¼ºç‰ˆæ™ºèƒ½ä½“ Demo                            â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
    
    # 1. åŠ è½½å‘é‡æ•°æ®åº“
    print("ğŸ“‚ åŠ è½½å‘é‡æ•°æ®åº“...")
    vector_store = VectorStore()
    vector_store.load_vector('./trial_bge')
    print(f"âœ… åŠ è½½å®Œæˆï¼Œæ–‡æ¡£æ•°é‡: {len(vector_store.document)}\n")
    
    # 2. åˆå§‹åŒ–æ¨¡å‹
    print("ğŸ¤– åˆå§‹åŒ–æ¨¡å‹...")
    embedding = BGEEmbedding()
    llm = OpenAIChat()
    print("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ\n")
    
    # 3. åˆ›å»ºå¢å¼ºç‰ˆæ™ºèƒ½ä½“
    print("ğŸš€ åˆ›å»ºå¢å¼ºç‰ˆæ™ºèƒ½ä½“...")
    agent = EnhancedRAGAgent(
        vector_store=vector_store,
        llm=llm,
        embedding=embedding,
        enable_tracking=True  # å¯ç”¨Tokenè¿½è¸ª
    )
    print("âœ… æ™ºèƒ½ä½“åˆ›å»ºå®Œæˆ\n")
    
    # 4. æµ‹è¯•é—®é¢˜
    test_questions = [
        "2024 å¹´ï¼Œæ ªæ´²ä¸­è½¦æ—¶ä»£ç”µæ°”è‚¡ä»½æœ‰é™å…¬å¸ä¸­æ ‡é‡‘é¢æ˜¯å¤šå°‘ï¼Ÿ",
        "æ ¹æ®æ¬§æ´²é“è·¯å±€ 2025-2027 å¹´å•ä¸€è§„åˆ’æ–‡ä»¶ï¼Œæœºæ„æ³¨å†Œç³»ç»Ÿè¿ç§»åˆ°çŸ¥è¯†å›¾è°±ï¼ˆknowledge graphï¼‰æ–¹æ³•çš„ç›®æ ‡è¿›åº¦åœ¨ 2025 å¹´åº•åº”è¾¾åˆ°å¤šå°‘ç™¾åˆ†æ¯”ï¼Ÿ",
        "åœ¨ UIC çš„æŠ¥å‘Šé‡Œï¼Œæ ¹æ®å›½é™…èƒ½æºç½²çš„åˆ†æï¼Œé“è·¯çš„å¸‚åœºä»½é¢éœ€è¦å¢é•¿å¤šå°‘æ‰èƒ½åœ¨æœ¬åå¹´å†…å®ç°ã€Šå·´é»åå®šã€‹çš„ç›®æ ‡?",
        "å‚ç…§ IEEE 1474.1 çš„å®šä¹‰ï¼Œé™„ä»¶ D (Typical safe braking model) ä¸­å¯¹å®‰å…¨åˆ¶åŠ¨æ¨¡å‹çš„æè¿°ï¼Œåœ¨'æ»‘è¡Œæ—¶é—´ (Coast time, C)'æœŸé—´ï¼Œåˆ—è½¦è¢«å‡å®šå¤„äºä»€ä¹ˆçŠ¶æ€ï¼Ÿ",
        "åœ¨ Manresa è½¦ç«™çš„äº‹ä»¶è°ƒæŸ¥æŠ¥å‘Šä¸­ï¼Œåˆ—è½¦ 78443 è¢«æˆæƒè¶Šè¿‡ 3023 è¿›ç«™ä¿¡å·æœºåï¼Œä½•æ—¶ï¼ˆæ—¥æœŸå’Œæ—¶é—´ï¼‰å‘ç”Ÿäº†åˆ—è½¦ 95218 æœ€ç»ˆå¯åŠ¨äº†è¡Œé©¶ï¼Œå¹¶æœ€ç»ˆå¯¼è‡´ä¸¤åˆ—è½¦å­˜åœ¨ç¢°æ’é£é™©ï¼Ÿ",
        
        "å—äº¬åœ°é“ S7 å·çº¿çš„è¿è¥é‡Œç¨‹ï¼Œåœ¨æ±Ÿè‹çœå†…å·²è¿è¥åœ°é“é•¿åº¦ä¸­æ’ç¬¬å‡ ï¼Ÿ",
        "è½¦è¾†å¤–éƒ¨ç§»åŠ¨å®ä½“çš„åœºæ™¯è¦ç´ ï¼šæ ¹æ® GB/T 43267â€”2023ï¼ˆé¢„æœŸåŠŸèƒ½å®‰å…¨ï¼‰ï¼Œåœ¨åœºæ™¯è¦ç´ ç»“æ„ä¸­ï¼Œå¯ç§»åŠ¨å®ä½“çš„ç¬¬ 2 å±‚è¦ç´ å’Œç¬¬ 3 å±‚è¦ç´ åˆ†åˆ«æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆéœ€å®Œæ•´åˆ—å‡ºç¬¬ 3 å±‚ä¸­æ‰€æœ‰å®ä½“ç±»å‹ï¼‰ã€‚",
        "æ ¹æ®æ–‡æ¡£ã€Š2024_Communications-Based Train Controlã€‹ï¼Œå›¾ 5.11 æ‰€ç¤ºçš„ç½‘çŠ¶æ§åˆ¶å›è·¯ç»“æ„ï¼ŒATO å­ç³»ç»Ÿæ˜¯å¦‚ä½•å®ç°è‡ªèº«çš„æ§åˆ¶å›è·¯çš„ï¼Ÿè¯·é˜è¿°å…¶å¦‚ä½•è·å–è¾“å…¥ï¼ˆMessgliederï¼‰ï¼Œå¦‚ä½•å½¢æˆè½¦è¾†è½¨è¿¹ï¼ˆFahrzeugtrajektorieï¼‰ï¼Œä»¥åŠå¦‚ä½•å°†è½¨è¿¹ä½œä¸ºç›®æ ‡å€¼ä¼ é€’ç»™åˆ—è½¦çš„æ§åˆ¶è®¾å¤‡ï¼ˆSteuergerÃ¤tï¼‰ã€‚",

        "åœ¨ CBTC äº’è”äº’é€šè§„èŒƒä½“ç³»ä¸­ï¼Œå…³äºåˆ—è½¦å¯åŠ¨ã€åŠ é€Ÿã€å·¡èˆªå’Œåˆ¶åŠ¨çš„è‡ªåŠ¨æ§åˆ¶åŠŸèƒ½ï¼Œå…¶åœ¨ã€Šç³»ç»Ÿæ€»ä½“è¦æ±‚ã€‹ä¸­çš„åˆ†é…å½’å±äºå“ªä¸ªå­ç³»ç»Ÿï¼Ÿå¹¶åœ¨ã€ŠCBTC éƒ¨åˆ†æµ‹è¯•åŠéªŒè¯ã€‹ä¸­ä½“ç°åœ¨å“ªä¸ªåŠŸèƒ½çš„æµ‹è¯•ä¸­ï¼Œæµ‹è¯•éœ€æ±‚ç¼–å·æ˜¯ä»€ä¹ˆï¼Ÿ",
        "ERTMS/ETCS åˆ—è½¦ç‰µå¼•ç³»ç»Ÿæ•°æ®å®šä¹‰æ¼”å˜ï¼š æ¯”è¾ƒ SUBSET-026 Baseline 3 (v3.4.0) å’Œ Baseline 4 (v4.0.0) ç‰ˆæœ¬ä¸­ Validated Train Data (Packet 11) çš„å†…å®¹å®šä¹‰ï¼š1ï¼‰è¯·æŒ‡å‡ºè¯¥æ•°æ®åŒ…ä¸­ç”¨äºè¡¨ç¤ºç‰µå¼•ç³»ç»Ÿæ ‡è¯†çš„å˜é‡åç§°ï¼Ÿ2ï¼‰å½“è¯¥å˜é‡ä¸ä¸ºé›¶æ—¶ï¼Œéœ€è¦åŒ…å«å“ªäº›é¢å¤–çš„ç‰µå¼•æ•°æ®å˜é‡ï¼Ÿ",
        ]
    
    results = []
    start_index = 0

    # å°è¯•åŠ è½½å·²æœ‰çš„ç»“æœï¼Œä»¥ä¾¿ä»æ–­ç‚¹æ¢å¤
    if os.path.exists(RESULTS_FILE):
        try:
            with open(RESULTS_FILE, 'r', encoding='utf-8') as f:
                results = json.load(f)
            print(f"âœ… å·²åŠ è½½ {len(results)} ä¸ªå†å²ç»“æœï¼Œå°†ä»ä¸‹ä¸€é¢˜ç»§ç»­ã€‚")
        except json.JSONDecodeError:
            print("âš ï¸ å†å²ç»“æœæ–‡ä»¶æŸåï¼Œå°†é‡æ–°å¼€å§‹ã€‚")
            results = []
    
    # ä»ä¸Šæ¬¡ç»“æŸçš„ä½ç½®å¼€å§‹å¤„ç†æ–°é—®é¢˜
    start_index = len(results)

    for i in range(start_index, len(test_questions)):
        question = test_questions[i]
        print(f"\n============================================================")
        print(f"é—®é¢˜ {i+1}: {question}")
        print(f"============================================================\n")
        full_result_text = None
        try:
            # è°ƒç”¨æ™ºèƒ½ä½“ï¼Œè·å–ç»“æœ
            full_result_text = agent.query_with_full_features(question)
            
            # å­˜å‚¨å½“å‰é—®é¢˜çš„ç»“æœ
            results.append({
                "question_id": i + 1,
                "question": question,
                "answer": full_result_text,
                "status": "Success"
            })
            
        except Exception as e:
            # æ•è·é”™è¯¯ï¼Œè®°å½•ä¸‹æ¥ï¼Œå¹¶ç»§ç»­ä¸‹ä¸€ä¸ªé—®é¢˜
            print(f"âŒ é—®é¢˜ {i+1} å‘ç”Ÿé”™è¯¯ï¼Œæ— æ³•è·å–ç­”æ¡ˆ: {e}")
            results.append({
                "question_id": i + 1,
                "question": question,
                "answer": None,
                "status": f"Error: {str(e)}"
            })
            
        finally:
            # âš ï¸ æ¯æ¬¡å¾ªç¯ç»“æŸéƒ½ä¿å­˜ç»“æœï¼ˆå¢é‡ä¿å­˜ï¼‰
            with open(RESULTS_FILE, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=4)
            print(f"âœ… é—®é¢˜ {i+1} çš„ç»“æœå·²ä¿å­˜åˆ° {RESULTS_FILE}")

    print("\n\næ‰€æœ‰é—®é¢˜å¤„ç†å®Œæ¯•ã€‚")
    
    
    # for i, question in enumerate(test_questions, 1):
    #     print(f"\n{'='*60}")
    #     print(f"é—®é¢˜ {i}: {question}")
    #     print(f"{'='*60}\n")
        
    #     # 5. æ‰§è¡ŒæŸ¥è¯¢ï¼ˆä½¿ç”¨å®Œæ•´å¢å¼ºåŠŸèƒ½ï¼‰
    #     result = agent.query_with_full_features(question)
        
    #     # 6. æ˜¾ç¤ºç­”æ¡ˆ
    #     print(f"\nâœ… ç­”æ¡ˆ:")
    #     print(f"{result['answer']}\n")
        
    #     # 7. æ˜¾ç¤ºæ¨ç†é“¾ï¼ˆå¯é€‰ï¼‰
    #     if result.get('reasoning_chain'):
    #         print("\n" + "="*60)
    #         print("æ¨ç†è¿‡ç¨‹:")
    #         print("="*60)
    #         print(result['reasoning_chain'].format_chain(detailed=False))
        
    #     # 8. æ˜¾ç¤ºTokenä½¿ç”¨æƒ…å†µ
    #     if result.get('token_usage'):
    #         print(f"\nğŸ“Š æœ¬æ¬¡æŸ¥è¯¢Tokenæ¶ˆè€—:")
    #         print(f"  â€¢ æ€»è®¡: {result['token_usage']['total_tokens']:,} tokens")
        
    #     # 9. æ ¼å¼åŒ–è¾“å‡ºï¼ˆç¬¦åˆç«èµ›è¦æ±‚ï¼‰
    #     formatted_output = agent.format_output(result, include_reasoning=False)
    #     results.append(formatted_output)
        
    #     print("\n" + "="*60)
    
    # # 10. ä¿å­˜ç»“æœ
    # print("\nğŸ’¾ ä¿å­˜ç»“æœ...")
    # final_output = {"items": results}
    # with open('enhanced_demo_results.json', 'w', encoding='utf-8') as f:
    #     json.dump(final_output, f, ensure_ascii=False, indent=2) # å†™å…¥ final_output
    # print("âœ… ç»“æœå·²ä¿å­˜åˆ°: enhanced_demo_results.json\n")
    # ...
    # 11. æ˜¾ç¤ºæ€§èƒ½æŠ¥å‘Š
    print("\n" + "="*60)
    print("æ€§èƒ½æŠ¥å‘Š")
    print("="*60)
    print(agent.get_performance_report())
    
    # 12. ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    print("\nğŸ’¾ ä¿å­˜è¯¦ç»†æŠ¥å‘Š...")
    agent.save_reports(output_dir='enhanced_reports')
    
    print("\nâœ… Demoå®Œæˆï¼")


def demo_advanced_features():
    """æ¼”ç¤ºé«˜çº§åŠŸèƒ½"""
    print("\n\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘         é«˜çº§åŠŸèƒ½æ¼”ç¤º                                  â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
    
    # æ¼”ç¤º1ï¼šTokenä¼˜åŒ–
    print("ã€åŠŸèƒ½1ï¼šTokenä¼˜åŒ–ã€‘")
    from token_tracker import TokenTracker
    
    tracker = TokenTracker()
    long_text = "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„æ–‡æœ¬ã€‚\n" * 500
    print(f"åŸå§‹æ–‡æœ¬: {len(long_text)} å­—ç¬¦, {tracker.count_tokens(long_text)} tokens")
    
    optimized = tracker.optimize_context(long_text, max_tokens=200)
    print(f"ä¼˜åŒ–å: {len(optimized)} å­—ç¬¦, {tracker.count_tokens(optimized)} tokens")
    print(f"èŠ‚çœ: {len(long_text) - len(optimized)} å­—ç¬¦\n")
    
    # æ¼”ç¤º2ï¼šæ¨ç†é“¾
    print("ã€åŠŸèƒ½2ï¼šæ¨ç†é“¾è®°å½•ã€‘")
    from reasoning_chain import ReasoningChain
    
    chain = ReasoningChain("æµ‹è¯•é—®é¢˜")
    chain.add_analysis_step("åˆ†æé—®é¢˜ç±»å‹")
    chain.add_retrieval_step("æ£€ç´¢ç›¸å…³æ–‡æ¡£")
    chain.add_inference_step("è¿›è¡Œé€»è¾‘æ¨ç†", confidence=0.9)
    chain.add_conclusion_step("å¾—å‡ºç»“è®º")
    
    print(chain.format_compact())
    print()
    
    # æ¼”ç¤º3ï¼šè¡¨æ ¼æå–
    print("ã€åŠŸèƒ½3ï¼šè¡¨æ ¼æå–ã€‘")
    from advanced_document_processor import AdvancedTableExtractor
    
    extractor = AdvancedTableExtractor()
    print("è¡¨æ ¼æå–å™¨å·²åˆå§‹åŒ–")
    print("æ”¯æŒpdfplumberå’ŒPyMuPDFä¸¤ç§æ–¹æ³•\n")
    
    # æ¼”ç¤º4ï¼šç‰ˆæœ¬å¯¹æ¯”
    print("ã€åŠŸèƒ½4ï¼šç‰ˆæœ¬å¯¹æ¯”ã€‘")
    from advanced_document_processor import VersionComparator
    
    comparator = VersionComparator()
    doc1 = "ç‰ˆæœ¬1çš„å†…å®¹\nåŒ…å«åŠŸèƒ½A\nåŒ…å«åŠŸèƒ½B"
    doc2 = "ç‰ˆæœ¬2çš„å†…å®¹\nåŒ…å«åŠŸèƒ½A\nåŒ…å«åŠŸèƒ½B\næ–°å¢åŠŸèƒ½C"
    
    comparison = comparator.compare_documents(doc1, doc2)
    print(f"ç›¸ä¼¼åº¦: {comparison['similarity_ratio']:.1%}")
    print(f"æ–°å¢: {comparison['added_lines']}è¡Œ")
    print(f"åˆ é™¤: {comparison['removed_lines']}è¡Œ\n")


if __name__ == "__main__":
    try:
        # è¿è¡Œä¸»Demo
        main()
        
        # è¿è¡Œé«˜çº§åŠŸèƒ½æ¼”ç¤º
        demo_advanced_features()
        
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        print("\næç¤ºï¼šè¯·ç¡®ä¿:")
        print("1. .envæ–‡ä»¶é…ç½®æ­£ç¡®")
        print("2. å‘é‡æ•°æ®åº“å·²æ„å»º (./storage_demo)")
        print("3. æ‰€æœ‰ä¾èµ–å·²å®‰è£…")
        import traceback
        traceback.print_exc()

