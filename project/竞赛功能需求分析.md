# ç«èµ›åŠŸèƒ½éœ€æ±‚å…¨é¢åˆ†æ

## ğŸ¯ ç«èµ›æ ¸å¿ƒè¦æ±‚æ‹†è§£

### 1. çŸ¥è¯†åº“å¤„ç†èƒ½åŠ›
- âœ… **æ‰«æPDFå¤„ç†** - æå–æ–‡æœ¬ã€è¯†åˆ«ç»“æ„
- âœ… **å¤šç‰ˆæœ¬æ–‡æ¡£ç®¡ç†** - ç‰ˆæœ¬è¯†åˆ«ã€å¯¹æ¯”
- âœ… **ä¸­è‹±æ–‡æ··åˆå¤„ç†** - åŒè¯­æ”¯æŒ
- âœ… **ç»“æ„åŒ–è¡¨æ ¼æå–** - è¡¨æ ¼æ£€æµ‹ã€æ•°æ®æå–

### 2. æ ¸å¿ƒæ™ºèƒ½ä»»åŠ¡
- âœ… **è·¨æºå¯¹æ¯”åˆ†æ** - å¤šæ–‡æ¡£ä¿¡æ¯æ•´åˆ
- âœ… **æ¼”å˜è¿½æº¯** - ç‰ˆæœ¬å·®å¼‚åˆ†æ
- âœ… **é€»è¾‘æ¨ç†** - å¤šè½®æ¨ç†ã€å› æœåˆ†æ

### 3. è¯„æµ‹ä¼˜åŒ–ç›®æ ‡
- âœ… **ç­”æ¡ˆè´¨é‡** (æœ€é‡è¦ï¼Œæƒé‡æœ€é«˜)
- âœ… **Tokenæ¶ˆè€—** (æˆæœ¬ä¼˜åŒ–)
- âœ… **æ¨¡å‹å‚æ•°é‡** (æ•ˆç‡è€ƒé‡)

---

## ğŸ“Š å·²å®ç°åŠŸèƒ½ vs å¾…å®Œå–„åŠŸèƒ½

### âœ… å·²å®ç°çš„æ ¸å¿ƒåŠŸèƒ½

| åŠŸèƒ½æ¨¡å— | å®ç°çŠ¶æ€ | æ–‡ä»¶ä½ç½® | å®Œæˆåº¦ |
|---------|---------|---------|--------|
| **æ–‡æ¡£è¯»å–ä¸åˆ†å—** | âœ… å®Œæˆ | `utils.py`, `enhanced_utils.py` | 90% |
| **PDFå¢å¼ºå¤„ç†** | âœ… å®Œæˆ | `enhanced_utils.py` | 85% |
| **è¡¨æ ¼æ£€æµ‹æå–** | âœ… åŸºç¡€å®ç° | `enhanced_utils.py` | 70% |
| **å‘é‡å­˜å‚¨ä¸æ£€ç´¢** | âœ… å®Œæˆ | `VectorBase.py` | 95% |
| **é—®é¢˜ç±»å‹è¯†åˆ«** | âœ… å®Œæˆ | `agent.py` | 90% |
| **å¤šæŸ¥è¯¢å¢å¼º** | âœ… å®Œæˆ | `agent.py` | 85% |
| **é‡æ’åºæœºåˆ¶** | âœ… å®Œæˆ | `agent.py` | 80% |
| **åˆ†å±‚æç¤ºè¯** | âœ… å®Œæˆ | `agent.py`, `config.py` | 90% |
| **å¤šè½®æ£€ç´¢** | âœ… å®Œæˆ | `agent.py` | 85% |
| **è´¨é‡æ£€æŸ¥ä¸é‡è¯•** | âœ… å®Œæˆ | `agent.py` | 80% |
| **å…ƒæ•°æ®è¿½è¸ª** | âœ… å®Œæˆ | `enhanced_utils.py` | 85% |
| **æ‰¹é‡å¤„ç†** | âœ… å®Œæˆ | `run_competition.py` | 95% |

### âš ï¸ éœ€è¦å®Œå–„çš„åŠŸèƒ½

| åŠŸèƒ½æ¨¡å— | ä¼˜å…ˆçº§ | å½“å‰çŠ¶æ€ | å»ºè®®æ”¹è¿› |
|---------|-------|---------|---------|
| **OCRæ‰«æPDFå¤„ç†** | ğŸ”´ é«˜ | æœªå®ç° | æ·»åŠ OCRæ”¯æŒ |
| **è¡¨æ ¼æ·±åº¦è§£æ** | ğŸ”´ é«˜ | åŸºç¡€ç‰ˆ | å¢å¼ºè¡¨æ ¼ç»“æ„è¯†åˆ« |
| **å¤šè¯­è¨€å¤„ç†** | ğŸŸ¡ ä¸­ | åŸºç¡€æ”¯æŒ | ä¼˜åŒ–ä¸­è‹±æ–‡æ··åˆ |
| **ç‰ˆæœ¬å·®å¼‚å¯è§†åŒ–** | ğŸŸ¡ ä¸­ | æœªå®ç° | ç»“æ„åŒ–å¯¹æ¯”è¾“å‡º |
| **æ¨ç†é“¾è®°å½•** | ğŸŸ¡ ä¸­ | æœªå®ç° | è®°å½•æ¨ç†æ­¥éª¤ |
| **Tokenç»Ÿè®¡ä¸ä¼˜åŒ–** | ğŸŸ¡ ä¸­ | æœªå®ç° | å®æ—¶ç›‘æ§æ¶ˆè€— |
| **ç¼“å­˜æœºåˆ¶** | ğŸŸ¢ ä½ | æœªå®ç° | å‡å°‘é‡å¤è°ƒç”¨ |

---

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½è¯¦ç»†å®ç°æ–¹æ¡ˆ

### åŠŸèƒ½1ï¼šæ‰«æPDFå¤„ç†ï¼ˆOCRæ”¯æŒï¼‰â­â­â­

**é—®é¢˜ï¼š** éƒ¨åˆ†PDFæ˜¯æ‰«æç‰ˆï¼Œæ— æ³•ç›´æ¥æå–æ–‡æœ¬

**å½“å‰çŠ¶æ€ï¼š**
```python
# enhanced_utils.py åªæ”¯æŒå¯å¤åˆ¶æ–‡æœ¬çš„PDF
text = page.get_text("text")  # å¯¹æ‰«æPDFæ— æ•ˆ
```

**æ”¹è¿›æ–¹æ¡ˆï¼š**
```python
import pytesseract
from pdf2image import convert_from_path

def read_pdf_with_ocr(file_path: str) -> str:
    """æ”¯æŒOCRçš„PDFè¯»å–"""
    try:
        # å…ˆå°è¯•ç›´æ¥æå–æ–‡æœ¬
        doc = fitz.open(file_path)
        text = doc[0].get_text()
        
        # å¦‚æœæ–‡æœ¬ä¸ºç©ºæˆ–å¾ˆå°‘ï¼Œä½¿ç”¨OCR
        if len(text.strip()) < 50:
            print(f"æ£€æµ‹åˆ°æ‰«æPDFï¼Œä½¿ç”¨OCRå¤„ç†: {file_path}")
            return ocr_pdf(file_path)
        else:
            return standard_pdf_extract(file_path)
    except:
        return ocr_pdf(file_path)

def ocr_pdf(file_path: str) -> str:
    """OCRå¤„ç†æ‰«æPDF"""
    images = convert_from_path(file_path)
    full_text = []
    
    for i, image in enumerate(images):
        # ä½¿ç”¨ä¸­è‹±æ–‡OCR
        text = pytesseract.image_to_string(
            image, 
            lang='chi_sim+eng'  # ä¸­è‹±æ–‡æ··åˆ
        )
        full_text.append(f"\n[ç¬¬{i+1}é¡µ OCR]\n{text}")
    
    return "\n".join(full_text)
```

**ä¼˜å…ˆçº§ï¼š** ğŸ”´ é«˜ - å¾ˆå¤šæŠ€æœ¯è§„èŒƒå¯èƒ½æ˜¯æ‰«æç‰ˆ

---

### åŠŸèƒ½2ï¼šè¡¨æ ¼æ·±åº¦è§£æâ­â­â­

**é—®é¢˜ï¼š** å½“å‰è¡¨æ ¼æ£€æµ‹æ˜¯å¯å‘å¼çš„ï¼Œå‡†ç¡®ç‡ä¸å¤Ÿé«˜

**å½“å‰å®ç°ï¼š**
```python
# enhanced_utils.py - ç®€å•çš„ç©ºæ ¼æ£€æµ‹
if re.search(r'\s{2,}|\t', line):
    potential_table.append(line)
```

**æ”¹è¿›æ–¹æ¡ˆ1ï¼šä½¿ç”¨ä¸“ä¸šè¡¨æ ¼æå–åº“**
```python
import pdfplumber
import camelot

def extract_tables_advanced(file_path: str, page_num: int):
    """é«˜çº§è¡¨æ ¼æå–"""
    tables = []
    
    # æ–¹æ³•1ï¼špdfplumberï¼ˆå¿«é€Ÿï¼Œé€‚åˆç®€å•è¡¨æ ¼ï¼‰
    with pdfplumber.open(file_path) as pdf:
        page = pdf.pages[page_num]
        page_tables = page.extract_tables()
        
        for table in page_tables:
            # è½¬æ¢ä¸ºMarkdownæ ¼å¼ä¾¿äºLLMç†è§£
            markdown_table = convert_to_markdown(table)
            tables.append(markdown_table)
    
    # æ–¹æ³•2ï¼šcamelotï¼ˆç²¾ç¡®ï¼Œé€‚åˆå¤æ‚è¡¨æ ¼ï¼‰
    if not tables:
        try:
            camelot_tables = camelot.read_pdf(
                file_path, 
                pages=str(page_num+1),
                flavor='lattice'  # æˆ– 'stream'
            )
            for table in camelot_tables:
                df = table.df
                markdown_table = df.to_markdown()
                tables.append(markdown_table)
        except:
            pass
    
    return tables

def convert_to_markdown(table_data):
    """å°†è¡¨æ ¼è½¬ä¸ºMarkdownæ ¼å¼"""
    if not table_data:
        return ""
    
    markdown = []
    # è¡¨å¤´
    markdown.append("| " + " | ".join(str(cell) for cell in table_data[0]) + " |")
    markdown.append("|" + "|".join(["---" for _ in table_data[0]]) + "|")
    
    # è¡¨æ ¼å†…å®¹
    for row in table_data[1:]:
        markdown.append("| " + " | ".join(str(cell) for cell in row) + " |")
    
    return "\n".join(markdown)
```

**ä¼˜å…ˆçº§ï¼š** ğŸ”´ é«˜ - ç«èµ›é¢˜ç›®æ¶‰åŠå¤§é‡ç»Ÿè®¡è¡¨æ ¼

---

### åŠŸèƒ½3ï¼šå¤šç‰ˆæœ¬æ–‡æ¡£å¯¹æ¯”å¢å¼ºâ­â­â­

**éœ€æ±‚ï¼š** "æ¼”å˜è¿½æº¯"ä»»åŠ¡éœ€è¦æ¸…æ™°å±•ç¤ºç‰ˆæœ¬å·®å¼‚

**æ”¹è¿›æ–¹æ¡ˆï¼š**
```python
class VersionComparator:
    """ç‰ˆæœ¬å¯¹æ¯”å™¨"""
    
    def compare_versions(self, query: str, doc_v1: str, doc_v2: str):
        """å¯¹æ¯”ä¸¤ä¸ªç‰ˆæœ¬"""
        # 1. æå–ç‰ˆæœ¬å·
        v1_info = self.extract_version_info(doc_v1)
        v2_info = self.extract_version_info(doc_v2)
        
        # 2. ç»“æ„åŒ–å¯¹æ¯”
        comparison = {
            'version_1': v1_info,
            'version_2': v2_info,
            'differences': self.find_differences(doc_v1, doc_v2),
            'evolution_summary': None
        }
        
        # 3. ç”Ÿæˆæ¼”å˜åˆ†æ
        comparison['evolution_summary'] = self.generate_evolution_analysis(
            comparison, query
        )
        
        return comparison
    
    def find_differences(self, doc_v1: str, doc_v2: str):
        """æŸ¥æ‰¾å·®å¼‚ç‚¹"""
        # ä½¿ç”¨difflibè¿›è¡Œæ–‡æœ¬å¯¹æ¯”
        import difflib
        
        differ = difflib.Differ()
        diff = list(differ.compare(
            doc_v1.splitlines(), 
            doc_v2.splitlines()
        ))
        
        differences = {
            'added': [],      # v2ä¸­æ–°å¢çš„
            'removed': [],    # v1ä¸­åˆ é™¤çš„
            'modified': []    # ä¿®æ”¹çš„
        }
        
        for line in diff:
            if line.startswith('+ '):
                differences['added'].append(line[2:])
            elif line.startswith('- '):
                differences['removed'].append(line[2:])
        
        return differences
    
    def structure_comparison_context(self, comparison, query):
        """æ„å»ºç»“æ„åŒ–å¯¹æ¯”ä¸Šä¸‹æ–‡"""
        context = f"""
=== ç‰ˆæœ¬å¯¹æ¯”åˆ†æ ===

ã€ç‰ˆæœ¬1ä¿¡æ¯ã€‘
{comparison['version_1']}

ã€ç‰ˆæœ¬2ä¿¡æ¯ã€‘
{comparison['version_2']}

ã€ä¸»è¦å·®å¼‚ã€‘
æ–°å¢å†…å®¹ ({len(comparison['differences']['added'])} å¤„):
{self.format_changes(comparison['differences']['added'][:5])}

åˆ é™¤å†…å®¹ ({len(comparison['differences']['removed'])} å¤„):
{self.format_changes(comparison['differences']['removed'][:5])}

ã€é’ˆå¯¹é—®é¢˜çš„åˆ†æã€‘
é—®é¢˜: {query}
"""
        return context
```

**ä¼˜å…ˆçº§ï¼š** ğŸ”´ é«˜ - é«˜çº§é¢˜å¿…éœ€

---

### åŠŸèƒ½4ï¼šæ¨ç†é“¾å¯è§†åŒ–â­â­

**éœ€æ±‚ï¼š** å±•ç¤ºæ¨ç†è¿‡ç¨‹ï¼Œæé«˜ç­”æ¡ˆå¯ä¿¡åº¦

**å®ç°æ–¹æ¡ˆï¼š**
```python
class ReasoningChain:
    """æ¨ç†é“¾è®°å½•å™¨"""
    
    def __init__(self):
        self.steps = []
    
    def add_step(self, step_type: str, description: str, evidence: str):
        """æ·»åŠ æ¨ç†æ­¥éª¤"""
        self.steps.append({
            'step': len(self.steps) + 1,
            'type': step_type,
            'description': description,
            'evidence': evidence
        })
    
    def format_chain(self):
        """æ ¼å¼åŒ–æ¨ç†é“¾"""
        chain = ["=== æ¨ç†è¿‡ç¨‹ ===\n"]
        for step in self.steps:
            chain.append(f"æ­¥éª¤{step['step']}: {step['description']}")
            chain.append(f"  ä¾æ®: {step['evidence'][:100]}...")
            chain.append("")
        return "\n".join(chain)

# åœ¨agent.pyä¸­ä½¿ç”¨
def advanced_retrieve_with_reasoning(self, query: str):
    """å¸¦æ¨ç†é“¾çš„é«˜çº§æ£€ç´¢"""
    reasoning = ReasoningChain()
    
    # æ­¥éª¤1ï¼šè¯†åˆ«é—®é¢˜ç±»å‹
    query_type = self.analyze_query_type(query)
    reasoning.add_step(
        "é—®é¢˜åˆ†æ",
        f"è¯†åˆ«ä¸º{query_type}ç±»å‹é—®é¢˜",
        f"å…³é”®è¯: {self.extract_technical_terms(query)}"
    )
    
    # æ­¥éª¤2ï¼šç¬¬ä¸€è½®æ£€ç´¢
    results_r1 = self.multi_query_retrieve(query, k=10)
    reasoning.add_step(
        "ä¿¡æ¯æ£€ç´¢",
        f"ç¬¬ä¸€è½®æ£€ç´¢è·å¾—{len(results_r1)}ä¸ªç›¸å…³ç‰‡æ®µ",
        f"æ¥æº: {set([r['query'] for r in results_r1])}"
    )
    
    # æ­¥éª¤3ï¼šæå–å…³é”®ä¿¡æ¯
    key_entities = self.extract_key_entities_from_results(results_r1)
    reasoning.add_step(
        "ä¿¡æ¯æå–",
        f"ä»æ£€ç´¢ç»“æœä¸­æå–å…³é”®å®ä½“",
        f"å®ä½“: {key_entities}"
    )
    
    # æ­¥éª¤4ï¼šäºŒæ¬¡æ£€ç´¢
    results_r2 = self.targeted_retrieve(key_entities, k=5)
    reasoning.add_step(
        "ç»†åŒ–æ£€ç´¢",
        f"é’ˆå¯¹å…³é”®å®ä½“è¿›è¡ŒäºŒæ¬¡æ£€ç´¢",
        f"è·å¾—{len(results_r2)}ä¸ªç²¾ç¡®ç‰‡æ®µ"
    )
    
    return results_r1 + results_r2, reasoning
```

**ä¼˜å…ˆçº§ï¼š** ğŸŸ¡ ä¸­ - æé«˜ç­”æ¡ˆå¯è§£é‡Šæ€§

---

### åŠŸèƒ½5ï¼šTokenç»Ÿè®¡ä¸ä¼˜åŒ–â­â­â­

**éœ€æ±‚ï¼š** è¯„æµ‹ç»´åº¦ä¹‹ä¸€ï¼Œéœ€è¦å®æ—¶ç›‘æ§å’Œä¼˜åŒ–

**å®ç°æ–¹æ¡ˆï¼š**
```python
import tiktoken

class TokenTracker:
    """Tokenä½¿ç”¨è¿½è¸ªå™¨"""
    
    def __init__(self):
        self.enc = tiktoken.get_encoding("cl100k_base")
        self.usage = {
            'embedding': 0,
            'retrieval_context': 0,
            'llm_input': 0,
            'llm_output': 0,
            'total': 0
        }
    
    def count_tokens(self, text: str) -> int:
        """è®¡ç®—tokenæ•°é‡"""
        return len(self.enc.encode(text))
    
    def track_embedding(self, texts: List[str]):
        """è¿½è¸ªEmbeddingæ¶ˆè€—"""
        tokens = sum(self.count_tokens(t) for t in texts)
        self.usage['embedding'] += tokens
        self.usage['total'] += tokens
        return tokens
    
    def track_llm(self, prompt: str, response: str):
        """è¿½è¸ªLLMæ¶ˆè€—"""
        input_tokens = self.count_tokens(prompt)
        output_tokens = self.count_tokens(response)
        
        self.usage['llm_input'] += input_tokens
        self.usage['llm_output'] += output_tokens
        self.usage['total'] += input_tokens + output_tokens
        
        return input_tokens, output_tokens
    
    def get_report(self):
        """ç”Ÿæˆä½¿ç”¨æŠ¥å‘Š"""
        return f"""
Tokenä½¿ç”¨ç»Ÿè®¡:
- Embedding: {self.usage['embedding']:,} tokens
- æ£€ç´¢ä¸Šä¸‹æ–‡: {self.usage['retrieval_context']:,} tokens
- LLMè¾“å…¥: {self.usage['llm_input']:,} tokens
- LLMè¾“å‡º: {self.usage['llm_output']:,} tokens
- æ€»è®¡: {self.usage['total']:,} tokens
- é¢„ä¼°æˆæœ¬: ${self.usage['total'] * 0.000002:.4f}
"""
    
    def optimize_context(self, context: str, max_tokens: int = 3000):
        """ä¼˜åŒ–ä¸Šä¸‹æ–‡é•¿åº¦"""
        current_tokens = self.count_tokens(context)
        
        if current_tokens <= max_tokens:
            return context
        
        # æˆªæ–­ç­–ç•¥ï¼šä¿ç•™å¼€å¤´å’Œç»“å°¾ï¼Œå»é™¤ä¸­é—´
        lines = context.split('\n')
        target_lines = int(len(lines) * (max_tokens / current_tokens))
        
        keep_head = target_lines // 2
        keep_tail = target_lines // 2
        
        optimized = (
            '\n'.join(lines[:keep_head]) +
            "\n\n... [ä¸­é—´å†…å®¹å·²çœç•¥ä»¥èŠ‚çœtoken] ...\n\n" +
            '\n'.join(lines[-keep_tail:])
        )
        
        return optimized

# é›†æˆåˆ°agentä¸­
class RAGAgentWithTracking(RAGAgent):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.token_tracker = TokenTracker()
    
    def query_with_tracking(self, query: str):
        """å¸¦Tokenè¿½è¸ªçš„æŸ¥è¯¢"""
        # æ£€ç´¢é˜¶æ®µ
        results, context = self.retrieve(query)
        self.token_tracker.track_embedding([query])
        
        # ä¼˜åŒ–ä¸Šä¸‹æ–‡
        optimized_context = self.token_tracker.optimize_context(
            context, 
            max_tokens=3000
        )
        
        # LLMç”Ÿæˆ
        prompt = self.build_prompt(query, optimized_context)
        answer = self.llm.chat(prompt, [], optimized_context)
        
        # è¿½è¸ª
        self.token_tracker.track_llm(prompt, answer)
        
        # è¿”å›ç»“æœå’Œç»Ÿè®¡
        return {
            'answer': answer,
            'token_usage': self.token_tracker.usage,
            'optimization_applied': len(context) != len(optimized_context)
        }
```

**ä¼˜å…ˆçº§ï¼š** ğŸ”´ é«˜ - è¯„æµ‹å…³é”®æŒ‡æ ‡

---

### åŠŸèƒ½6ï¼šæ™ºèƒ½ç¼“å­˜æœºåˆ¶â­â­

**ç›®æ ‡ï¼š** å‡å°‘é‡å¤APIè°ƒç”¨ï¼Œé™ä½æˆæœ¬

**å®ç°æ–¹æ¡ˆï¼š**
```python
import hashlib
import json
import os

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = './cache'):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        
        self.embedding_cache_file = f"{cache_dir}/embeddings.json"
        self.llm_cache_file = f"{cache_dir}/llm_responses.json"
        
        self.embedding_cache = self.load_cache(self.embedding_cache_file)
        self.llm_cache = self.load_cache(self.llm_cache_file)
    
    def load_cache(self, file_path: str):
        """åŠ è½½ç¼“å­˜"""
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        with open(self.embedding_cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.embedding_cache, f, ensure_ascii=False)
        
        with open(self.llm_cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.llm_cache, f, ensure_ascii=False)
    
    def get_hash(self, text: str) -> str:
        """ç”Ÿæˆæ–‡æœ¬å“ˆå¸Œ"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def get_embedding(self, text: str, embedding_model):
        """è·å–Embeddingï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        cache_key = self.get_hash(text)
        
        if cache_key in self.embedding_cache:
            return self.embedding_cache[cache_key]
        
        # è°ƒç”¨API
        vector = embedding_model.get_embedding(text)
        
        # ç¼“å­˜
        self.embedding_cache[cache_key] = vector
        self.save_cache()
        
        return vector
    
    def get_llm_response(self, prompt: str, llm_model):
        """è·å–LLMå“åº”ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        cache_key = self.get_hash(prompt)
        
        if cache_key in self.llm_cache:
            print("  [ä½¿ç”¨ç¼“å­˜]")
            return self.llm_cache[cache_key]
        
        # è°ƒç”¨API
        response = llm_model.chat(prompt, [], "")
        
        # ç¼“å­˜
        self.llm_cache[cache_key] = response
        self.save_cache()
        
        return response
```

**ä¼˜å…ˆçº§ï¼š** ğŸŸ¡ ä¸­ - å¼€å‘é˜¶æ®µå¾ˆæœ‰ç”¨

---

### åŠŸèƒ½7ï¼šå¤šè¯­è¨€å¤„ç†ä¼˜åŒ–â­â­

**éœ€æ±‚ï¼š** å¤„ç†ä¸­è‹±æ–‡æ··åˆçš„æŠ€æœ¯è§„èŒƒ

**æ”¹è¿›æ–¹æ¡ˆï¼š**
```python
def optimize_multilingual_processing(query: str, context: str):
    """ä¼˜åŒ–ä¸­è‹±æ–‡æ··åˆå¤„ç†"""
    
    # 1. æ£€æµ‹è¯­è¨€æ¯”ä¾‹
    import langdetect
    
    def detect_language_ratio(text):
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_words = len(re.findall(r'[a-zA-Z]+', text))
        total = chinese_chars + english_words
        
        return {
            'chinese_ratio': chinese_chars / total if total > 0 else 0,
            'english_ratio': english_words / total if total > 0 else 0
        }
    
    lang_info = detect_language_ratio(context)
    
    # 2. æ ¹æ®è¯­è¨€æ¯”ä¾‹è°ƒæ•´æç¤ºè¯
    if lang_info['english_ratio'] > 0.5:
        # è‹±æ–‡ä¸ºä¸»
        prompt_prefix = "The following context is primarily in English. "
    elif lang_info['chinese_ratio'] > 0.5:
        # ä¸­æ–‡ä¸ºä¸»
        prompt_prefix = "ä»¥ä¸‹ä¸Šä¸‹æ–‡ä¸»è¦æ˜¯ä¸­æ–‡ã€‚"
    else:
        # æ··åˆ
        prompt_prefix = "ä»¥ä¸‹ä¸Šä¸‹æ–‡åŒ…å«ä¸­è‹±æ–‡æ··åˆå†…å®¹ã€‚è¯·æ³¨æ„å‡†ç¡®ç†è§£ä¸¤ç§è¯­è¨€çš„æœ¯è¯­ã€‚"
    
    # 3. ä¸“ä¸šæœ¯è¯­å¯¹ç…§
    term_mapping = {
        'CBTC': 'åŸºäºé€šä¿¡çš„åˆ—è½¦æ§åˆ¶',
        'ATO': 'åˆ—è½¦è‡ªåŠ¨è¿è¡Œ',
        'ATP': 'åˆ—è½¦è‡ªåŠ¨é˜²æŠ¤',
        # ... æ›´å¤šæœ¯è¯­
    }
    
    # åœ¨æç¤ºè¯ä¸­æ·»åŠ æœ¯è¯­è¯´æ˜
    if any(term in query or term in context for term in term_mapping):
        prompt_prefix += "\n\næœ¯è¯­å¯¹ç…§:\n"
        for en, zh in term_mapping.items():
            if en in query or en in context:
                prompt_prefix += f"- {en} = {zh}\n"
    
    return prompt_prefix
```

**ä¼˜å…ˆçº§ï¼š** ğŸŸ¡ ä¸­

---

## ğŸ“‹ å®Œæ•´åŠŸèƒ½æ¸…å•ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

### ğŸ”´ å¿…é¡»å®ç°ï¼ˆå½±å“ç­”æ¡ˆè´¨é‡ï¼‰

1. âœ… **åŸºç¡€RAGåŠŸèƒ½** - å·²å®Œæˆ
2. âœ… **é—®é¢˜ç±»å‹è¯†åˆ«** - å·²å®Œæˆ
3. âœ… **å¤šæŸ¥è¯¢å¢å¼º** - å·²å®Œæˆ
4. âœ… **é‡æ’åºæœºåˆ¶** - å·²å®Œæˆ
5. âš ï¸ **OCRæ”¯æŒ** - éœ€æ·»åŠ ï¼ˆå¤„ç†æ‰«æPDFï¼‰
6. âš ï¸ **é«˜çº§è¡¨æ ¼æå–** - éœ€å¢å¼ºï¼ˆä½¿ç”¨pdfplumber/camelotï¼‰
7. âš ï¸ **ç‰ˆæœ¬å¯¹æ¯”åŠŸèƒ½** - éœ€å®Œå–„ï¼ˆç»“æ„åŒ–å¯¹æ¯”ï¼‰
8. âš ï¸ **Tokenè¿½è¸ªä¸ä¼˜åŒ–** - éœ€å®ç°ï¼ˆè¯„æµ‹æŒ‡æ ‡ï¼‰

### ğŸŸ¡ é‡è¦å®ç°ï¼ˆæå‡ç«äº‰åŠ›ï¼‰

9. âœ… **åˆ†å±‚æç¤ºè¯** - å·²å®Œæˆ
10. âœ… **å¤šè½®æ£€ç´¢** - å·²å®Œæˆ
11. âš ï¸ **æ¨ç†é“¾è®°å½•** - éœ€å®ç°ï¼ˆæé«˜å¯è§£é‡Šæ€§ï¼‰
12. âš ï¸ **æ™ºèƒ½ç¼“å­˜** - éœ€å®ç°ï¼ˆé™ä½æˆæœ¬ï¼‰
13. âš ï¸ **å¤šè¯­è¨€ä¼˜åŒ–** - éœ€å¢å¼º

### ğŸŸ¢ å¯é€‰å®ç°ï¼ˆé”¦ä¸Šæ·»èŠ±ï¼‰

14. âš ï¸ **ç­”æ¡ˆç½®ä¿¡åº¦è¯„åˆ†**
15. âš ï¸ **è‡ªåŠ¨åŒ–æµ‹è¯•æ¡†æ¶**
16. âš ï¸ **æ€§èƒ½ç›‘æ§ä»ªè¡¨ç›˜**

---

## ğŸ¯ æ¨èå®ç°è·¯çº¿å›¾

### ç¬¬ä¸€é˜¶æ®µï¼šæ ¸å¿ƒåŠŸèƒ½å®Œå–„ï¼ˆ3-5å¤©ï¼‰

**ç›®æ ‡ï¼š** ç¡®ä¿åŸºç¡€åŠŸèƒ½ç¨³å®šï¼Œèƒ½å¤„ç†æ‰€æœ‰ç±»å‹çš„æ–‡æ¡£

```bash
Day 1-2: OCRæ”¯æŒ + é«˜çº§è¡¨æ ¼æå–
Day 3-4: Tokenè¿½è¸ªä¸ä¼˜åŒ–
Day 5: ç‰ˆæœ¬å¯¹æ¯”åŠŸèƒ½å®Œå–„
```

### ç¬¬äºŒé˜¶æ®µï¼šæ™ºèƒ½å¢å¼ºï¼ˆ2-3å¤©ï¼‰

**ç›®æ ‡ï¼š** æå‡ç­”æ¡ˆè´¨é‡å’Œç³»ç»Ÿæ™ºèƒ½åº¦

```bash
Day 6-7: æ¨ç†é“¾è®°å½• + å¤šè¯­è¨€ä¼˜åŒ–
Day 8: æ™ºèƒ½ç¼“å­˜æœºåˆ¶
```

### ç¬¬ä¸‰é˜¶æ®µï¼šä¼˜åŒ–è°ƒè¯•ï¼ˆ2-3å¤©ï¼‰

**ç›®æ ‡ï¼š** è°ƒä¼˜å‚æ•°ï¼Œæµ‹è¯•æ‰€æœ‰é¢˜ç›®

```bash
Day 9-10: è¿è¡Œæ‰€æœ‰ç«èµ›é¢˜ç›®ï¼Œè°ƒä¼˜å‚æ•°
Day 11: æœ€ç»ˆæµ‹è¯•å’Œæ–‡æ¡£æ•´ç†
```

---

## ğŸ’¡ å¿«é€Ÿå¼€å§‹æŒ‡å—

### ç«‹å³å¯ç”¨çš„åŠŸèƒ½
```bash
# 1. ä½¿ç”¨ç°æœ‰åŠŸèƒ½å¤„ç†ç«èµ›é¢˜ç›®
cd /home/honglianglu/ssd/casco/project
python run_competition.py --mode all

# 2. æŸ¥çœ‹ç»“æœ
cat outputs/all_results.json
```

### éœ€è¦æ·»åŠ çš„ä¾èµ–
```bash
# OCRæ”¯æŒ
pip install pytesseract pdf2image
sudo apt-get install tesseract-ocr tesseract-ocr-chi-sim

# é«˜çº§è¡¨æ ¼æå–
pip install pdfplumber camelot-py[cv]

# å…¶ä»–å·¥å…·
pip install langdetect difflib
```

### ä¼˜å…ˆå®ç°å»ºè®®
```python
# 1. å…ˆæ·»åŠ Tokenè¿½è¸ªï¼ˆæœ€ç®€å•ï¼Œæœ€é‡è¦ï¼‰
# åœ¨agent.pyä¸­é›†æˆTokenTracker

# 2. å¢å¼ºè¡¨æ ¼æå–ï¼ˆæå‡ç­”æ¡ˆè´¨é‡ï¼‰
# åœ¨enhanced_utils.pyä¸­ä½¿ç”¨pdfplumber

# 3. æ·»åŠ OCRæ”¯æŒï¼ˆå¤„ç†æ‰«æPDFï¼‰
# åœ¨enhanced_utils.pyä¸­æ·»åŠ OCRé€‰é¡¹
```

---

## âœ… æ€»ç»“

**ä½ å½“å‰çš„ç³»ç»Ÿå·²ç»å…·å¤‡äº†80%çš„æ ¸å¿ƒåŠŸèƒ½ï¼**

**å‰©ä½™20%å…³é”®æ”¹è¿›ï¼š**
1. ğŸ”´ OCRæ”¯æŒï¼ˆå¤„ç†æ‰«æPDFï¼‰
2. ğŸ”´ é«˜çº§è¡¨æ ¼æå–ï¼ˆæé«˜å‡†ç¡®ç‡ï¼‰
3. ğŸ”´ Tokenè¿½è¸ªä¼˜åŒ–ï¼ˆè¯„æµ‹æŒ‡æ ‡ï¼‰
4. ğŸŸ¡ ç‰ˆæœ¬å¯¹æ¯”å¢å¼ºï¼ˆé«˜çº§é¢˜ï¼‰
5. ğŸŸ¡ æ¨ç†é“¾è®°å½•ï¼ˆå¯è§£é‡Šæ€§ï¼‰

**å»ºè®®ä¼˜å…ˆçº§ï¼š**
- Tokenè¿½è¸ª > è¡¨æ ¼æå– > OCR > ç‰ˆæœ¬å¯¹æ¯” > æ¨ç†é“¾

**é¢„è®¡å®Œæˆæ—¶é—´ï¼š** 5-7å¤©å¯å®Œæˆæ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½

**æˆåŠŸå…³é”®ï¼š**
- ç­”æ¡ˆè´¨é‡ä¼˜å…ˆï¼ˆå è¯„åˆ†æœ€å¤§æƒé‡ï¼‰
- Tokenä¼˜åŒ–å…¶æ¬¡ï¼ˆå¹³è¡¡æˆæœ¬å’Œæ•ˆæœï¼‰
- ä¿æŒä»£ç ç®€æ´å¯ç»´æŠ¤

